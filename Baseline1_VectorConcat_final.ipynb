{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-g5MwiXSQ3s",
    "outputId": "5de6ccb6-0956-40f9-ee12-8b7f86dfbece"
   },
   "outputs": [],
   "source": [
    "print(\"\\nVerifying uploaded files in /content/:\")\n",
    "!ls -lh /content/*.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_Jo8nCIi1qZ",
    "outputId": "a9e87310-e230-4878-b93b-2f282ee6de2a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Choosing a folder\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/co_attention_flickr30k_new/features_vit_b16\"\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841,
     "referenced_widgets": [
      "11ec388f1adc411f892c194899f7b6d4",
      "6ff2dbf78b2a4e419e85d095c820be76",
      "0df7b512259b4761b697785e9944e90b",
      "2fbbb31016b045a293c536075fd2d0ec",
      "2df17f0517464fcb976ac58946a252d5",
      "2381c7c072be45b3b7f8ebeaf7858722",
      "0ae68197709e4346b118c796d1dbd288",
      "c2fdecf37db44d308b5103fbde6173da",
      "9786455ae8ce4e69bb4a080f405d698d",
      "e7286001eb394f6ab29cc6aeb8ebb9ea",
      "30957b838a054eb4943baf914e4d5651",
      "2e80367a923443a7b42415078d08e5d5",
      "65e93e4cf58d4cab80951b79de915d24",
      "88b74c12478b4d3cab7ab9fe8f3c5c49",
      "192ec82cf301462cbf14c2df32b9f4ab",
      "c2d0b914b61745568655f76f99069b02",
      "80f227f8cce541f782cd9f77bf03ec94",
      "e581f35e26134d7782ae21317669bc20",
      "103b72ebd6d34b769fdf4aaaab66cb2f",
      "18dceba135f5479ba52eba96d9333fd7",
      "782622e8edca413ba77a373fdae75340",
      "392defcf294144268d88b140573a289d",
      "c8bf1e202e534711a9e32d549fdfbd8c",
      "bbc63f59493d4ca19328a6efd7cdd45e",
      "c11cbe656c23424aa206d953c4eb297e",
      "c39e2a80aba944fc9fe2d5d8c8c18d5a",
      "ba79ac2c113c480d9ebc700fc1a4f05f",
      "4519a5e3ce854dcb880e18edc91146f3",
      "b86a1d2b994e431eb7e3cc3de51fd43b",
      "47d6e9ded3824cceb6aa9ad0020ab7df",
      "59cd0da32f80495cbca7424dc812e88d",
      "628ee90ad4dc4d6da05eb014a55077a4",
      "adbbc1dae06942689bdb3f7ecffaccc3",
      "6d2f270e44dc401e8aaf169a759b9f7f",
      "785ea71772004c778460fb84d61f1d81",
      "c88caea06b3645af88550029973b781d",
      "f38f29cac5f84898be09feab45775890",
      "1f13547bf8f348d6b4a063a1a64636ad",
      "77ef2af2811747819bfe80580dbb24b0",
      "2a75c45b8cc945d9b19021d3d44094c2",
      "5ebc72323912444490de0744f487b301",
      "d50e108d6b7c4cbeb829d0353b0a5e6b",
      "01baaad46eda453fa7281c991780ffb7",
      "8aea397043e14da481c0b8f1e1cd7b5f",
      "6508bf67b9f3478cbb7a9be3ad97f5d4",
      "fc4884dbcdd14d038ee36c3d5374a582",
      "d394fbff1eb7435eaeebbbbb6047712a",
      "b7769775759f4dd1923066732c06c3e8",
      "a488cf25bfc84ffbb5f0df9e2ed8deae",
      "120ee2455377404b985d7617d03936f6",
      "46e614cff59d43eea0cab67e6f5e23e7",
      "4e137fd2844c427d8c56a28677168b48",
      "324504c3b1014cab97fc1028b7195a8b",
      "c03faf6ed5c14485be166090bd2a05ec",
      "c3aa653ef5f04c8b84300ffff45e6a9b",
      "870c8355ba9e493caf22d91cd5263761",
      "05a3f39e93ad4305995c62a025ec8567",
      "8e124c509bcf4ee08833a553d2b1ba1a",
      "9210d42445ad47a18bff1722de25863d",
      "359fde13822d41d9810bf7d58a0562df",
      "bf303d1563144aa8b5c58c1c3bc6beaa",
      "57a1a2e1ea654d7fbb662d6d26423f10",
      "ca23db0835e748d193d367b757c5e6af",
      "91c06542994e4cd9878d8bc217d0da83",
      "d839171be4e0484ba6345acfd2f075a0",
      "35fffbc0116a42118e9abc9e9719fa07",
      "130e037b309942289ca058d1675bf81c",
      "6648afb29dc84c0b858c314df0a54c2b",
      "41ac59042d4d4d11b6b62a1a5e6595b0",
      "ec6722af47294dfdb4733b87ec41c9a9",
      "6cdf99082cd44fda9b8fdbbbbf072208",
      "6041ed2b96af4a32b45dcb8803e02326",
      "488e7b33f74148e7b6a45ce24f4e6b82",
      "3c311e5110ae4c02a11d74f7c952bd75",
      "2ac5c2ff4e764d5aac999c3f81587cfe",
      "3a9df8b915614a5781537b4f0e3f03fc",
      "96c2a2ab416b46f1a103fe2c6244f89c",
      "af9fb0543c53421db748cdacd9067fdf",
      "99b640d456454994923602278a34775b",
      "66e6cfa469524db1a9e20d0d0eb7d513",
      "2afe709e6da5447a90768a06188f981b",
      "aa8bf4ed91774a2e803cfef930c215d6",
      "4f4df2d514284f57965ae18d904d1e91",
      "f430b1372fe64785b49312428b5ab4ec",
      "de9f9057e6904a709869d99d79fb2ae2",
      "c88aba19c6d541fda05ea8d7b1629995",
      "1a0eccb0668a48f9883a8c312fd85397",
      "060000b11189424e92101234ecc75f73",
      "9ca2c1bbd2b044aeb026d8a194119166",
      "36a08f884ea94253888353e7eed60f79",
      "483a433457514ea0a431230fe2240118",
      "9a2a813882e4487794dfbb0a8e3cf8ba",
      "94d567b7eed04f8f8861fecb7c541c6a",
      "b82e502b5ed245a884fe51a1e60f6e1d",
      "f9ed39aafac044fdbcfd8ce75baf8986",
      "0e2552ebcb0b4a3dade24ac8396aa6bf",
      "ae72919bdebd4cb895a231fd09a72880",
      "20f51208c9b449b2ac203e515b9ed4d8",
      "e6df6a3b7aa3453cbdfa2b7fe2cc4541",
      "63cc613867f342918eff28c0a7020602",
      "f5923367940349a490dbc127a837abb5",
      "fc4e7254d4804ae595560844ea95294a",
      "446a656ddc9643a38a7fbca106c0cfd3",
      "2900ff3b591e49b7a78d3154afd9792c",
      "964a0adea8ac4049b1fe4e2eb2190e8d",
      "68947caa8aed45f5934fe013b2a0e610",
      "6d36289062f6405781f9a83f93182dfa",
      "115c42a48cd748d5b6c4ea0377dcf7dc",
      "9924868597984490ace1fa810a878462",
      "a3bbdfac01c940608eb8ca9dd48d26fd",
      "69ac2c909e264b4db17f0c7234feb845",
      "a3679317ae5f448bad5281f3f6c32710",
      "69de0d8cc6e740b3a1aa5f6db0ab25cf",
      "ff37a2146c744386a4262137a3df1da8",
      "6dc2884ba0674bb49470ba203c877655",
      "2676eedfa49c4e899bf5b4c90e6314c9",
      "776ef37861f84e239f49434af6d71ffa",
      "92e43f4c40ce4fe1a1f21b3b92ae9a67",
      "6cf9b5316bbc45769bfd3c8b70f705ce",
      "0f96d6f55b01428fa042473020a25c3b",
      "126f1570589f4b53a02d575b880cf680",
      "cc05ec482447496a8fe42474ca560604",
      "9c95f2d5b34b41898028b72ef704fe84",
      "4095188d5d1b407db571f34dca1c639a",
      "3f5c4a97700b47899f21fdb2700d960c",
      "7d9cb8735d0f4b7688d9bfc0b4d2fe92",
      "fb8f7a38b24d45768c7b73fcd2d64f7b",
      "3ca969e85be14e728ad6aaa594c35022",
      "90514c248e3544d4b2cb6902a9ada8ae",
      "f13caaad26c94784a8a38d821e75b96c",
      "f7022463e0af4901abc18d4f5fa7ae91",
      "e73b4760553b44c3a453aebeced2b270",
      "558fbbf55a9244b7b8ba6fff9431ae20",
      "3b293160f83a45a38639a1f77e0da99e",
      "c386810dd6eb4323804e6456bf9359c9",
      "649eb9e315fa4c7ab60174d1ea709976",
      "5d5bd63914c840ffb613aa609876bb41",
      "58ecaad3509c4af580ad5c188c63c81a",
      "e5f0f4ddb0414822a8eadbbaf9674a16",
      "c3ac5ba5f3c541c1856dcb4b8deac084",
      "ced966c1ba38489da849b42c69ca9788",
      "c34485628a3948399f2004a959061284",
      "cd55a58e679640178d4d3805a054ac86",
      "1632aa980df2483a8a4bbb23f327bf45",
      "26795a8ed52545b6b3ab422b4efdbd95",
      "aa763370173e49a7bbe1b0b58be76cf2",
      "3316db95a33845189acfebcedc1f383f",
      "89b2e98d3cfd4ac28f2917f6c5e8a188",
      "a224fce538d04c3e8c4cbfacb2f8cb1c",
      "98deef5509cf40ddabc5c19688b65bbd",
      "394759309a8445f9a6c9a8b4a2159640",
      "06ce1792763e4bc2a85c9148acc96af2",
      "fe4508e99e0b42f6b2b94669530c53ad",
      "769df5bf856b4854adac572b9aea07bc",
      "a9167c524cb94930958f8784d150f13d",
      "88120287bc534bdb915267990e051d15",
      "e3b4513f0c3242878b35a3e4c50b6a68",
      "554eacd40e7249fe97fb834b8c3d42ef",
      "9b78cb1b25844526a6a9e0c191f083e4",
      "a3707dd058c64b11aa70c43f76266023",
      "b3bf48fcbaa94b56974b95731fb547a6",
      "0eb9f33a20e54476a3a1c1a59cd4cd41",
      "cc18f01e754e4644ab149d65d3538872",
      "ccaee3bad7d84ab085fcff7b7f666e99",
      "77758ed9199d44f3ad4d6f8bc685926a",
      "95bd5579962f4348aac32562a60b47f3",
      "60572582a2cc4fadb95e36f5d1149c83",
      "68642a3431ba41f5b5f953e471872abd",
      "87ac756b30c34dceb0da3976deec9a78",
      "06934b051da6445689da3d1890769236",
      "45a95480dc1e403aacc1c31e3f8eca20",
      "1f33a828008940faa8bec716b4710d78",
      "4372e430843b4d8fb22c21fe66642796",
      "1a143a5711304068a3042262895acc16",
      "43fc586bead84c70915a71fa894f86a0",
      "2792a3be850f40aeac7309100d6fb372",
      "a4cd401c432245be9083c56f60cdf843",
      "a5c10261af7c4619946b5e5b67569dcf",
      "4fa991ac91c4466890eb7ab640480ad3",
      "1786806638be4fbaa0c76b3d09ef9db0",
      "b940cb992f1d462eb5764d51d56ecd3b",
      "cd3a6af9e34048d89763b0a3b6937b1b",
      "e37049fb2ca640caa1388d68eabc2e07",
      "ea8d9a24285e4b12957263c768794373",
      "62e05555001049c39c88a6cc3b559fe5",
      "a1779a871b0f4a88849458e234e93859",
      "25012dd9c32c4a70b5d2e61a961f9987"
     ]
    },
    "id": "Lq6bnPcaSXfX",
    "outputId": "ee29f1d8-5f3b-42a3-8f6d-dfcec925f9ed"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loading Captions using Direct Parquet Links\n",
    "print(\"Downloading Flickr30k data directly from Parquet files...\")\n",
    "\n",
    "#URLs point to preprocessed data files directly bypassing broken script\n",
    "DATA_FILES = [\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0000.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0001.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0002.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0003.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0004.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0005.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0006.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0007.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0008.parquet\",\n",
    "]\n",
    "\n",
    "# Loading all data as one big chunk first\n",
    "raw_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=DATA_FILES,\n",
    "    cache_dir=\"./hf_cache\"\n",
    ")[\"train\"]\n",
    "\n",
    "# Filtering them into splits using 'split' column provided in data\n",
    "flickr = {\n",
    "    \"train\": raw_dataset.filter(lambda x: x[\"split\"] == \"train\"),\n",
    "    \"validation\": raw_dataset.filter(lambda x: x[\"split\"] == \"val\"),\n",
    "    \"test\": raw_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "}\n",
    "\n",
    "print(f\"Data Loaded! Train: {len(flickr['train'])}, Val: {len(flickr['validation'])}, Test: {len(flickr['test'])}\")\n",
    "\n",
    "# Loading Image Features from local upload\n",
    "print(\"\\nLoading Image Features into RAM...\")\n",
    "base_path = \"/content\"\n",
    "\n",
    "# Ensuring catching errors if files aren't uploaded\n",
    "try:\n",
    "    print(\"Loading Global Features...\")\n",
    "    img_feats_train = torch.load(os.path.join(PROJECT_ROOT, \"flickr30k_train_global.pt\"))\n",
    "    img_feats_val   = torch.load(os.path.join(PROJECT_ROOT, \"flickr30k_val_global.pt\"))\n",
    "    img_feats_test  = torch.load(os.path.join(PROJECT_ROOT, \"flickr30k_test_global.pt\"))\n",
    "\n",
    "    print(f\"Train: {img_feats_train.shape}\") # [29000, 768]\n",
    "    print(f\"Val:   {img_feats_val.shape}\")   # [1014, 768]\n",
    "    print(f\"Test:  {img_feats_test.shape}\")  # [1000, 768]\n",
    "\n",
    "    print(f\"Features Loaded. Train: {img_feats_train.shape}, Val: {img_feats_val.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find the .pt files. Please make sure you uploaded them in Cell 1.\")\n",
    "\n",
    "# Preparing tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgKPz5YHSbNp",
    "outputId": "458905d1-ace7-4d73-89d8-0cb71c926bd4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Flickr30kRetrievalDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_feats, tokenizer, max_length=32, random_caption=True):\n",
    "        # Ensuring we have as many image vectors as we have captions\n",
    "        \n",
    "        self.ds = hf_dataset\n",
    "        self.img_feats = img_feats\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.random_caption = random_caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # getting Image feature\n",
    "        img_feat = self.img_feats[idx] # Shape [768]\n",
    "\n",
    "        # getting caption\n",
    "        \n",
    "        captions = self.ds[idx][\"caption\"]\n",
    "\n",
    "        if self.random_caption:\n",
    "            caption = random.choice(captions)\n",
    "        else:\n",
    "            caption = captions[0]\n",
    "\n",
    "        # Tokenizing\n",
    "        tok = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"img_feat\": img_feat,\n",
    "            \"input_ids\": tok[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Creating Loaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ret = Flickr30kRetrievalDataset(\n",
    "    flickr[\"train\"], \n",
    "    img_feats_train,\n",
    "    tokenizer,\n",
    "    max_length=MAX_LEN,\n",
    "    random_caption=True\n",
    ")\n",
    "\n",
    "val_ret = Flickr30kRetrievalDataset(\n",
    "    hf_dataset=flickr[\"validation\"],  #Passing object directly\n",
    "    img_feats=img_feats_val,          # Must match size of flickr[\"validation\"]\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LEN,\n",
    "    random_caption=False\n",
    ")\n",
    "\n",
    "test_ret = Flickr30kRetrievalDataset(\n",
    "    flickr[\"test\"], \n",
    "    img_feats_test,\n",
    "    tokenizer,\n",
    "    max_length=MAX_LEN,\n",
    "    random_caption=True\n",
    ")\n",
    "\n",
    "# train=29k, val=1k, test=1k\n",
    "# ensuring img_feats_train length matches dataset split length\n",
    "print(f\"Dataset Size: {len(train_ret)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ret, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ret, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ret, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330,
     "referenced_widgets": [
      "223ec6aa3c2c4062b3c97e67c7ae7343",
      "acf2bd42324d4b99b4ac8227f2e010c0",
      "2fecad9a6c674e6ba0689b7137a21b90",
      "d21004e1191f43a69af34cbf59939dba",
      "70d5454a33444856bab353fc4ce5bf8d",
      "0e9767161d0b442eb7bb13c42acc4dd0",
      "d02d4db437344540ab2700b22f9c4d3e",
      "bd929b0ecee94a758ee243d29eae2eca",
      "2b56d01b2f1d43a6be91de380e7a7936",
      "62560a28b0334aec843b42457595eb93",
      "f0b8d612635a496db01f445956b501e3",
      "080b8ca0443246caa257e46da0085501",
      "5dcc3710a7c24e4e9d8d1d7d1b40fc88",
      "f2a38b73b2544bed885c43f18087174e",
      "ea1ec4124bde4d9abd9be2b86a2c238a",
      "3506e6659f0e4ac1b4ae0e3dd8bedb02",
      "4fdbed143d7140c6bd541366d98d1670",
      "888d5861060549b5a3bcb5ba4dcafacb",
      "299744e2eef948ada18f00e87957739b",
      "090735c2cb094fe99cd870cd98ee448e",
      "007cf03e957245689111da334bcc7d0c",
      "88280a578fe643ddb2dac61993b9ee33",
      "33443d8d1a08422e84b29931d27964f5",
      "097041cd28454c7bb4008d5edf377136",
      "0a1227d3ccaf432fa36ad8feee0195c8",
      "9c5e746fd9d0494e80c71995eff5cfc9",
      "19ce734948da49459935ad866d9891be",
      "592c20a511a44784933607efc3e10ba9",
      "a0b0411c97174531bce2dc66580cfcd1",
      "cf90ca7def564ddaa514758c4e3bfd3b",
      "bdb9ccf5630d46959c55336366d53853",
      "3558d574eb514265a488a4c95b951396",
      "8fe97690bc554535b33b58567c981e8b",
      "8539b5d74e814851ad1b4483c8b5f968",
      "0fbaeed08022479ebbda594d45cdecdf",
      "60350d08f45e45e3862b7ba0ec134db0",
      "23b2c60bbcd146b08d3582520a8eb0cd",
      "2c637aaee41443488d76bd55ffdaec14",
      "f5438b0a8f27469aa90f7575c30cbaa4",
      "a7b413de8b6441bb8d5d55dae302ffda",
      "8c40e59af1c44dd6a47c713bf2788392",
      "ea78ea538ec94f879b610c3ffb10b3f7",
      "8f46e56ac6a64494aaf0c9951450268e",
      "ed80a8e42bbf4f8fa6054de981dcde94",
      "75a1ab02fa3349e1abbc75c0591731fe",
      "9ce5fe3aa4d845b2a869faa3edf60ace",
      "111a620515244c4ebdf0dd03854d40dd",
      "d002e700941b4e8593d2f75889462c16",
      "2e40c231c1ba4793a4ee0ca79961f7c2",
      "5c86e600210f411b889053ba2bfee575",
      "fd52578e12704be2ab72fd8f81267b7c",
      "d0ab603a85c748679c3c15d328989205",
      "6921ee44b58f454d90ced02804b93ec6",
      "774957e51e5a470582339547c7c3998e",
      "ca4f0b78b28745fc8932612b48a16474",
      "8b210def2f154ed59cbc1197534597a9",
      "8ab4e6397ccc4b41b3ab8cc52ce5c779",
      "b83087152b5d4b1aa08b710c01c7570c",
      "df6750442fae4297817b5da7e0ac5d0d",
      "f91ef3994cdf4e12a1760a366d4ab779",
      "d18bc462f04a46eeaf64551df07f9467",
      "e547202545374370b760d88611f0addd",
      "52436e5fb25549ae8db8bc909bc3b4e3",
      "006fbcd7e3ca469b88fed6d91f495a57",
      "9ca1ab83320642d7b0d2183d2507e517",
      "3d288091dfc9440699dbd981024777e0"
     ]
    },
    "id": "NfMEuhejSdKX",
    "outputId": "57db6e03-669c-49ac-e59e-4dea6c50d589"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# defining model\n",
    "class Baseline1_Concat(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(Baseline1_Concat, self).__init__()\n",
    "\n",
    "        # Frozen BERT\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Fusion MLP, (768 img + 768 text) --> 1 score\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(768 + 768, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feats, input_ids, attention_mask):\n",
    "        # Text Embeddings [B, 768]\n",
    "        with torch.no_grad():\n",
    "            text_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            text_vec = text_out.pooler_output\n",
    "\n",
    "        # Concat [B,1536]\n",
    "        fused = torch.cat((img_feats, text_vec), dim=1)\n",
    "\n",
    "        # Score [B]\n",
    "        return self.fusion_mlp(fused).squeeze(1)\n",
    "\n",
    "# Setting up training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on: {DEVICE}\")\n",
    "\n",
    "model = Baseline1_Concat().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MarginRankingLoss(margin=0.2)\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Running Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        # Moving to GPU\n",
    "        img = batch[\"img_feat\"].to(DEVICE)\n",
    "        txt = batch[\"input_ids\"].to(DEVICE)\n",
    "        msk = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # Positive Scores\n",
    "        pos_scores = model(img, txt, msk)\n",
    "\n",
    "        # Negative Scores. Shifting captions by 1 to create mismatches\n",
    "        txt_neg = torch.roll(txt, shifts=1, dims=0)\n",
    "        msk_neg = torch.roll(msk, shifts=1, dims=0)\n",
    "        neg_scores = model(img, txt_neg, msk_neg)\n",
    "\n",
    "        # Loss\n",
    "        ones = torch.ones(img.size(0)).to(DEVICE)\n",
    "        loss = criterion(pos_scores, neg_scores, ones)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lSg3SRzo4y6q",
    "outputId": "3c9e3647-2230-4b2c-a3b5-713eca82ca8d"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLL3j3HR4N7j",
    "outputId": "8dd5235b-2693-42d8-ee20-d633b88b2e4a"
   },
   "outputs": [],
   "source": [
    "def get_fixed_test_subset(loader, device, num_samples=200):\n",
    "    \"\"\"\n",
    "    This extracts first N samples from loader to create fixed evaluation set\n",
    "    \"\"\"\n",
    "    all_imgs = []\n",
    "    all_ids = []\n",
    "    all_masks = []\n",
    "\n",
    "    collected = 0\n",
    "    print(f\" Extracting fixed subset of {num_samples} samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Handling Dictionary vs Tuple\n",
    "            if isinstance(batch, dict):\n",
    "                img = batch['img_feat']\n",
    "                ids = batch['input_ids']\n",
    "                mask = batch['attention_mask']\n",
    "            else:\n",
    "                img, ids, mask = batch\n",
    "\n",
    "            all_imgs.append(img)\n",
    "            all_ids.append(ids)\n",
    "            all_masks.append(mask)\n",
    "\n",
    "            collected += img.size(0)\n",
    "            if collected >= num_samples:\n",
    "                break\n",
    "\n",
    "    # Concatenating and triming exactly to num_samples\n",
    "    subset = {\n",
    "        \"img\": torch.cat(all_imgs)[:num_samples].to(device),\n",
    "        \"ids\": torch.cat(all_ids)[:num_samples].to(device),\n",
    "        \"mask\": torch.cat(all_masks)[:num_samples].to(device),\n",
    "        \"N\": num_samples\n",
    "    }\n",
    "\n",
    "    print(f\"Fixed Test Subset Ready. (N={num_samples})\")\n",
    "    return subset\n",
    "\n",
    "test_subset = get_fixed_test_subset(test_loader, device, num_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289,
     "referenced_widgets": [
      "acf138e844f0416f9b855cffac05773f",
      "a32e5a198e1f4b098675aaf0591d3464",
      "7dbbabdeebf049a8be11b7f7a5e10c44",
      "caf563e63f564c8caf7ca6be29d74d42",
      "387d868f7c6e4469bd2a494b210e0999",
      "1d218aee0e314c3caf115ee3eb9735a5",
      "e4d48308c05f4d3eae5adf53e1bfd7c1",
      "9432f227ca5f4678a899c0c026be5a29",
      "ddfac562445b492bbed5223ab7b5824f",
      "644b86e39cb44297b042e721864969d3",
      "9333aa676eba405493b1265543b700a7",
      "1ce6336314464c318c3a3bc18a88e375",
      "30918d4b5bf748988b86b84d5470b785",
      "554b712fbde14896ab767b18cb6126aa",
      "731cd2b0b5ef4a148159d51ed1199f44",
      "7bf99b35e00f4b289e6fc1cc7e63a04f",
      "dbbde6bcfefc426ea4191f7dc319b969",
      "421229a3a65a4da3a7f5958b57f1a900",
      "64e4fb8af55f4e9ebdbfaa3c44998d0b",
      "72d3d0b1d28a4ea8b2d424c466d6bb1b",
      "6739c3e2d80f45d9b251ac63f8f0b42e",
      "f7ef42ec8c904ff99d92e307424b0b6b"
     ]
    },
    "id": "9e7NX4snfFsO",
    "outputId": "4c1e3f35-db70-4811-f6ae-d37bd7d21839"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_baseline_concat(model, test_subset, device):\n",
    "    model.eval()\n",
    "\n",
    "    print(\"--- Step 1: Extracting all features ---\")\n",
    "    all_img_feats = []\n",
    "    all_text_feats = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_subset, desc=\"Extracting Embeddings\"):\n",
    "            # Storing image features, which are already pre computed by ViT\n",
    "            img = batch[\"img_feat\"].to(device)\n",
    "            all_img_feats.append(img.cpu())\n",
    "\n",
    "            # computing text features\n",
    "            txt_input = batch[\"input_ids\"].to(device)\n",
    "            txt_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # BERT inside model directly\n",
    "            text_out = model.bert(input_ids=txt_input, attention_mask=txt_mask)\n",
    "\n",
    "\n",
    "            text_vec = text_out.pooler_output\n",
    "            all_text_feats.append(text_vec.cpu())\n",
    "\n",
    "\n",
    "    img_tensor = torch.cat(all_img_feats, dim=0).to(device)\n",
    "    txt_tensor = torch.cat(all_text_feats, dim=0).to(device)\n",
    "\n",
    "    n_samples = img_tensor.size(0)\n",
    "    print(f\"\\nEvaluated sizes: Images {img_tensor.shape}, Texts {txt_tensor.shape}\")\n",
    "\n",
    "    print(\"\\n--- Step 2: Computing 1M Scores (The Slow Part) ---\")\n",
    "    # need matrix [N_images, N_texts]\n",
    "    sim_matrix = torch.zeros((n_samples, n_samples))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Looping over every image, query\n",
    "        for i in tqdm(range(n_samples), desc=\"Scoring All Pairs\"):\n",
    "            # Getting single image vector [1,768]\n",
    "            img_vec = img_tensor[i].unsqueeze(0)\n",
    "\n",
    "            #Repeating it to match all text vectors [N,768]\n",
    "            img_repeated = img_vec.repeat(n_samples, 1)\n",
    "\n",
    "            #Concatenating [N,68] + [N,768] --> [N,1536]\n",
    "            #pairs current image with every caption in dataset\n",
    "            fused_input = torch.cat((img_repeated, txt_tensor), dim=1)\n",
    "\n",
    "            # Runing MLP, fusion layer\n",
    "            # calling fusion_mlp directly to bypass BERT step\n",
    "            scores = model.fusion_mlp(fused_input) #[N,1]\n",
    "            scores = scores.squeeze(1)             # [N]\n",
    "\n",
    "            #Storing in matrix\n",
    "            sim_matrix[i] = scores.cpu()\n",
    "\n",
    "    print(\"\\n--- Step 3: Calculating Metrics ---\")\n",
    "    #calculating recall for image to Text\n",
    "    print(\">> Image-to-Text (Given Image, find Caption)\")\n",
    "    i2t_r1, i2t_r5, i2t_r10 = calculate_recall(sim_matrix, direction=\"i2t\")\n",
    "    print(f\"R@1: {i2t_r1:.2f}% | R@5: {i2t_r5:.2f}% | R@10: {i2t_r10:.2f}%\")\n",
    "\n",
    "    # calculating recall for text to image\n",
    "    print(\"\\n>> Text-to-Image (Given Caption, find Image)\")\n",
    "    t2i_r1, t2i_r5, t2i_r10 = calculate_recall(sim_matrix, direction=\"t2i\")\n",
    "    print(f\"R@1: {t2i_r1:.2f}% | R@5: {t2i_r5:.2f}% | R@10: {t2i_r10:.2f}%\")\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "def calculate_recall(sim_matrix, direction=\"i2t\"):\n",
    "    # standard Recall@K calculation\n",
    "    # matrix rows = queries,cols = targets\n",
    "\n",
    "    if direction == \"t2i\":\n",
    "        # if query is text,we transpose matrix\n",
    "        sim_matrix = sim_matrix.t()\n",
    "\n",
    "    n = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Score of correct pair, diagonal element\n",
    "        target_score = sim_matrix[i, i]\n",
    "\n",
    "        # Scores of all candidates for this query\n",
    "        row_scores = sim_matrix[i, :]\n",
    "\n",
    "\n",
    "        rank = (row_scores > target_score).sum().item() + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "\n",
    "    r1 = 100.0 * np.sum(ranks <= 1) / n\n",
    "    r5 = 100.0 * np.sum(ranks <= 5) / n\n",
    "    r10 = 100.0 * np.sum(ranks <= 10) / n\n",
    "\n",
    "    return r1, r5, r10\n",
    "\n",
    "\n",
    "sim_matrix_test = evaluate_baseline_concat(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OM58eJjKgE-v",
    "outputId": "e2a39347-f7e4-4daf-852b-d4512aa9ffcc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def visualize_retrieval(dataset, sim_matrix_test, idx=0):\n",
    "    \"\"\"\n",
    "    dataset: The Dataset object (val_ret) - NOT the loader\n",
    "    sim_matrix: The computed score matrix from Step 1\n",
    "    idx: The index of the image in the validation set you want to test\n",
    "    \"\"\"\n",
    "\n",
    "    #Setting up data\n",
    "    # Getting raw image from underlying HF dataset\n",
    "    # need PIL image, not tensor\n",
    "    example = dataset.ds[idx]\n",
    "    pil_img = example[\"image\"]\n",
    "\n",
    "    # Getting Ground Truth caption, the first one in the list\n",
    "    gt_caption = example[\"caption\"][0]\n",
    "\n",
    "    #Getting Predictions\n",
    "    # Grabbing scores for this image\n",
    "    scores = sim_matrix_test[idx]\n",
    "\n",
    "    # Getting top 3 indices\n",
    "    # argsort sorts ascending, so we take last 3 and reverse them\n",
    "    topk_indices = torch.argsort(scores, descending=True)[:3]\n",
    "\n",
    "    #Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plotting Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Query Image #{idx}\")\n",
    "\n",
    "    # Plotting Text\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    text_display = f\"GROUND TRUTH:\\n{textwrap.fill(gt_caption, width=50)}\\n\\n\"\n",
    "    text_display += \"PREDICTIONS:\\n\"\n",
    "\n",
    "    for rank, pred_idx in enumerate(topk_indices):\n",
    "        pred_idx = pred_idx.item()\n",
    "\n",
    "        # Getting text for that index from dataset\n",
    "        pred_caption = dataset.ds[pred_idx][\"caption\"][0]\n",
    "\n",
    "        # Checking if it is correct one\n",
    "        is_correct = (pred_idx == idx)\n",
    "        if is_correct:\n",
    "            marker = \"[ CORRECT ]\"\n",
    "        else:\n",
    "            marker = f\"[ WRONG - Img #{pred_idx} ]\"\n",
    "\n",
    "        text_display += f\"{rank+1}. {marker} {textwrap.fill(pred_caption, width=50)}\\n\"\n",
    "        text_display += f\"   (Score: {scores[pred_idx]:.4f})\\n\\n\"\n",
    "\n",
    "    plt.text(0, 0.5, text_display, fontsize=12, va='center')\n",
    "    plt.show()\n",
    "\n",
    "# executing visualization\n",
    "# Passing dataset (val_ret), not loader\n",
    "# Changing 'idx' to see different examples\n",
    "visualize_retrieval(val_ret, sim_matrix_test, idx=5)\n",
    "visualize_retrieval(val_ret, sim_matrix_test, idx=7)\n",
    "visualize_retrieval(val_ret, sim_matrix_test, idx=63)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
