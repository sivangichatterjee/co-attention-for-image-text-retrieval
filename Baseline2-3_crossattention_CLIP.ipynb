{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Universal Device Selector\n",
    "# On your Mac M4, this MUST print \"Using Device: mps\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "FEAT_DIR = os.path.join(PROJECT_ROOT, \"features\") \n",
    "\n",
    "print(f\"Looking for features in: {FEAT_DIR}\")\n",
    "\n",
    "try:\n",
    "    # Load the Global Features\n",
    "    # map_location=device ensures they load directly to the Mac's unified memory\n",
    "    img_feats_train = torch.load(os.path.join(FEAT_DIR, \"flickr30k_train_global.pt\"), map_location=device)\n",
    "    img_feats_val   = torch.load(os.path.join(FEAT_DIR, \"flickr30k_val_global.pt\"), map_location=device)\n",
    "    img_feats_test  = torch.load(os.path.join(FEAT_DIR, \"flickr30k_test_global.pt\"), map_location=device)\n",
    "    \n",
    "    print(\"Features Loaded Successfully!\")\n",
    "    print(f\"Train Shape: {img_feats_train.shape}\")\n",
    "    print(f\"Val Shape:   {img_feats_val.shape}\")\n",
    "    print(f\"Test Shape:  {img_feats_test.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nERROR: Files not found.\")\n",
    "    print(f\"I looked in: {FEAT_DIR}\")\n",
    "    print(\"Please create a folder named 'features' and put your .pt files inside it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading Flickr30k text data from Hugging Face...\")\n",
    "\n",
    "DATA_FILES = [\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0000.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0001.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0002.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0003.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0004.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0005.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0006.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0007.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0008.parquet\",\n",
    "]\n",
    "\n",
    "# Load and Split\n",
    "raw_dataset = load_dataset(\"parquet\", data_files=DATA_FILES, cache_dir=\"./hf_cache\")[\"train\"]\n",
    "\n",
    "flickr = {\n",
    "    \"train\": raw_dataset.filter(lambda x: x[\"split\"] == \"train\"),\n",
    "    \"validation\": raw_dataset.filter(lambda x: x[\"split\"] == \"val\"),\n",
    "    \"test\": raw_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "}\n",
    "\n",
    "print(f\"Text Loaded! Train: {len(flickr['train'])}, Val: {len(flickr['validation'])}, Test: {len(flickr['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_feats, tokenizer, max_len=32, random_cap=True):\n",
    "        self.ds = hf_dataset\n",
    "        self.img_feats = img_feats\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.random_cap = random_cap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_feats[idx]\n",
    "        captions = self.ds[idx]['caption']\n",
    "        \n",
    "        # Random caption during training helps the model learn better\n",
    "        txt = random.choice(captions) if self.random_cap else captions[0]\n",
    "        \n",
    "        tok = self.tokenizer(\n",
    "            txt, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"img_feat\": img,\n",
    "            \"input_ids\": tok[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Create DataLoaders (Batch Size 128 is good for M4)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    FlickrDataset(flickr['train'], img_feats_train, tokenizer, random_cap=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    FlickrDataset(flickr['validation'], img_feats_val, tokenizer, random_cap=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    FlickrDataset(flickr['test'], img_feats_test, tokenizer, random_cap=False),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"DataLoaders Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CrossAttentionInteractionModel(nn.Module):\n",
    "    def __init__(self, img_dim=768, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Text Encoder (Frozen)\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for p in self.bert.parameters(): \n",
    "            p.requires_grad = False \n",
    "        \n",
    "        # 2. Image Projection\n",
    "        self.img_proj = nn.Linear(img_dim, hidden_dim)\n",
    "        \n",
    "        # 3. Cross Attention\n",
    "        # batch_first=True is CRITICAL for shape (Batch, Seq, Dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # 4. Classifier\n",
    "        # Input is hidden_dim * 2 because we CONCATENATE (Image + Text_Context)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feat, input_ids, mask):\n",
    "        # --- A. Text Embeddings ---\n",
    "        with torch.no_grad():\n",
    "            # Shape: (Batch, Seq_Len, 768)\n",
    "            txt_seq = self.bert(input_ids, mask).last_hidden_state \n",
    "            \n",
    "        # --- B. Image Embeddings ---\n",
    "        # Shape: (Batch, 1, 768) - We treat image as a sequence of length 1\n",
    "        img_hidden = self.img_proj(img_feat).unsqueeze(1) \n",
    "        \n",
    "        # --- C. Cross Attention (The Fix) ---\n",
    "        # Query = Image (We are asking: \"What part of the text matches this image?\")\n",
    "        # Key/Value = Text (The source information)\n",
    "        \n",
    "        # Mask logic: PyTorch expects True where we want to IGNORE. \n",
    "        # BERT mask is 0 for padding. So we ignore where mask == 0.\n",
    "        key_padding_mask = (mask == 0)\n",
    "\n",
    "        attn_out, _ = self.cross_attn(\n",
    "            query=img_hidden,    # (Batch, 1, Dim)\n",
    "            key=txt_seq,         # (Batch, Seq, Dim)\n",
    "            value=txt_seq,       # (Batch, Seq, Dim)\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        # attn_out is (Batch, 1, Dim) -> The weighted average of text tokens that matched the image\n",
    "        \n",
    "        # --- D. Interaction ---\n",
    "        # CRITICAL: Concatenate the original Image with the Context from Text\n",
    "        # This lets the classifier compare \"What I have\" (Image) vs \"What I found\" (Text Context)\n",
    "        combined = torch.cat([img_hidden, attn_out], dim=-1).squeeze(1) # Shape: (Batch, 768*2)\n",
    "        \n",
    "        # Score\n",
    "        return self.classifier(combined).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CrossAttentionInteractionModel().to(device)\n",
    "\n",
    "# 2. Optimizer (Fresh start)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4  # Slightly safer LR for attention\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Starting Training (Corrected Logic)...\")\n",
    "\n",
    "# 3. Training Loop\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        img_feat = batch['img_feat'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Forward Pass (Positive) ---\n",
    "        # Target: 1.0\n",
    "        pos_scores = model(img_feat, input_ids, mask)\n",
    "        loss_pos = criterion(pos_scores, torch.ones_like(pos_scores))\n",
    "        \n",
    "        # --- Forward Pass (Negative) ---\n",
    "        # Target: 0.0\n",
    "        # Shuffle images to create mismatches (Image A + Text B)\n",
    "        neg_img_feat = img_feat[torch.randperm(img_feat.size(0))]\n",
    "        neg_scores = model(neg_img_feat, input_ids, mask)\n",
    "        loss_neg = criterion(neg_scores, torch.zeros_like(neg_scores))\n",
    "        \n",
    "        # --- Update ---\n",
    "        loss = loss_pos + loss_neg\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (Prevents exploding gradients in Attention)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # LIVE MONITORING\n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} Done. Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INVESTIGATION CELL ---\n",
    "batch = next(iter(train_loader))\n",
    "img_feat = batch['img_feat']\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "print(\"--- DATA INSPECTION ---\")\n",
    "print(f\"Image Feature Stats: Min={img_feat.min().item():.4f}, Max={img_feat.max().item():.4f}, Mean={img_feat.mean().item():.4f}\")\n",
    "print(f\"Are Image Features all Zero? {torch.all(img_feat == 0).item()}\")\n",
    "\n",
    "print(f\"Input IDs Stats: Min={input_ids.min().item()}, Max={input_ids.max().item()}\")\n",
    "print(f\"Are Input IDs all Zero? {torch.all(input_ids == 0).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_test_subset(loader, device, num_samples=200):\n",
    "    \"\"\"\n",
    "    Extracts the first N samples from the loader to create a fixed evaluation set.\n",
    "    \"\"\"\n",
    "    all_imgs = []\n",
    "    all_ids = []\n",
    "    all_masks = []\n",
    "    \n",
    "    collected = 0\n",
    "    print(f\" Extracting fixed subset of {num_samples} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Handle Dictionary vs Tuple\n",
    "            if isinstance(batch, dict):\n",
    "                img = batch['img_feat']\n",
    "                ids = batch['input_ids']\n",
    "                mask = batch['attention_mask']\n",
    "            else:\n",
    "                img, ids, mask = batch \n",
    "            \n",
    "            all_imgs.append(img)\n",
    "            all_ids.append(ids)\n",
    "            all_masks.append(mask)\n",
    "            \n",
    "            collected += img.size(0)\n",
    "            if collected >= num_samples:\n",
    "                break\n",
    "            \n",
    "    # Concatenate and trim exactly to num_samples\n",
    "    subset = {\n",
    "        \"img\": torch.cat(all_imgs)[:num_samples].to(device),\n",
    "        \"ids\": torch.cat(all_ids)[:num_samples].to(device),\n",
    "        \"mask\": torch.cat(all_masks)[:num_samples].to(device),\n",
    "        \"N\": num_samples\n",
    "    }\n",
    "    \n",
    "    print(f\"Fixed Test Subset Ready. (N={num_samples})\")\n",
    "    return subset\n",
    "\n",
    "test_subset = get_fixed_test_subset(test_loader, device, num_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cross_encoder(model, subset):\n",
    "    model.eval()\n",
    "    imgs = subset[\"img\"]\n",
    "    ids = subset[\"ids\"]\n",
    "    masks = subset[\"mask\"]\n",
    "    N = subset[\"N\"]\n",
    "    \n",
    "    print(f\"\\n--- EVALUATING CROSS-ENCODER (Baseline 3) ---\")\n",
    "    \n",
    "    r1, r5, r10, mrr = 0, 0, 0, 0\n",
    "    \n",
    "    # Iterate through each text query\n",
    "    for i in tqdm(range(N), desc=\"Ranking\"):\n",
    "        \n",
    "        # 1. Prepare Inputs: Repeat the i-th text N times\n",
    "        query_ids = ids[i].unsqueeze(0).repeat(N, 1)\n",
    "        query_mask = masks[i].unsqueeze(0).repeat(N, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 2. Predict Match Score for (Text_i, All_Images)\n",
    "            scores = model(imgs, query_ids, query_mask)\n",
    "        \n",
    "        # 3. Rank\n",
    "        # The correct image is at index 'i'. \n",
    "        # We sort scores descending (High score = good match)\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        # Find where 'i' is in the list\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item()\n",
    "        true_rank = rank + 1 # 1-based rank\n",
    "        \n",
    "        # Update Metrics\n",
    "        if true_rank == 1: r1 += 1\n",
    "        if true_rank <= 5: r5 += 1\n",
    "        if true_rank <= 10: r10 += 1\n",
    "        mrr += 1.0 / true_rank\n",
    "\n",
    "    print(f\"RESULTS (N={N}):\")\n",
    "    print(f\"R@1: {r1/N*100:.2f}% | R@5: {r5/N*100:.2f}% | MRR: {mrr/N:.4f}\")\n",
    "\n",
    "# --- ACTION: Run on Baseline 3 ---\n",
    "evaluate_cross_encoder(model, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_cross_encoder(model, subset, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Move subset tensors to device ONCE for efficiency\n",
    "    imgs = subset[\"img\"].to(device)\n",
    "    ids = subset[\"ids\"].to(device)\n",
    "    masks = subset[\"mask\"].to(device)\n",
    "    N = subset[\"N\"]\n",
    "    \n",
    "    print(f\"\\n--- EVALUATING CROSS-ENCODER (Baseline 3, N={N}) ---\")\n",
    "    \n",
    "    # Initialize metrics for both T2I and I2T\n",
    "    t2i_r1, t2i_r5, t2i_r10, t2i_mrr = 0, 0, 0, 0\n",
    "    i2t_r1, i2t_r5, i2t_r10, i2t_mrr = 0, 0, 0, 0\n",
    "    \n",
    "    # --- 1. Text-to-Image (T2I) Retrieval ---\n",
    "    # Query: Text (i) | Database: Images (j=0 to N-1)\n",
    "    print(\"\\n[T2I] Ranking Text queries against all Images...\")\n",
    "    for i in tqdm(range(N), desc=\"T2I Ranking\"):\n",
    "        \n",
    "        # 1. Prepare Inputs: Repeat the i-th text N times\n",
    "        query_ids = ids[i].unsqueeze(0).repeat(N, 1)\n",
    "        query_mask = masks[i].unsqueeze(0).repeat(N, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 2. Predict Match Score for (Text_i, All_Images)\n",
    "            # CRITICAL: We pass ALL images (imgs) and the repeated text\n",
    "            scores = model(imgs, query_ids, query_mask)\n",
    "        \n",
    "        # 3. Rank (Correct image is at index 'i')\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item()\n",
    "        true_rank = rank + 1\n",
    "        \n",
    "        # Update T2I Metrics\n",
    "        if true_rank == 1: t2i_r1 += 1\n",
    "        if true_rank <= 5: t2i_r5 += 1\n",
    "        if true_rank <= 10: t2i_r10 += 1\n",
    "        t2i_mrr += 1.0 / true_rank\n",
    "        \n",
    "    # --- 2. Image-to-Text (I2T) Retrieval ---\n",
    "    # Query: Image (i) | Database: Texts (j=0 to N-1)\n",
    "    print(\"\\n[I2T] Ranking Image queries against all Texts...\")\n",
    "    for i in tqdm(range(N), desc=\"I2T Ranking\"):\n",
    "        \n",
    "        # 1. Prepare Inputs: Repeat the i-th image N times\n",
    "        query_img = imgs[i].unsqueeze(0).repeat(N, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 2. Predict Match Score for (Image_i, All_Texts)\n",
    "            # CRITICAL: We pass ALL texts (ids, masks) and the repeated image\n",
    "            scores = model(query_img, ids, masks)\n",
    "        \n",
    "        # 3. Rank (Correct text is at index 'i')\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item()\n",
    "        true_rank = rank + 1\n",
    "        \n",
    "        # Update I2T Metrics\n",
    "        if true_rank == 1: i2t_r1 += 1\n",
    "        if true_rank <= 5: i2t_r5 += 1\n",
    "        if true_rank <= 10: i2t_r10 += 1\n",
    "        i2t_mrr += 1.0 / true_rank\n",
    "\n",
    "    # --- 3. Final Output ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"     FINAL RETRIEVAL RESULTS (N={N})\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"--- Text to Image (T2I) ---\")\n",
    "    print(f\"R@1:  {t2i_r1/N*100:.2f}% | R@5: {t2i_r5/N*100:.2f}% | R@10: {t2i_r10/N*100:.2f}%\")\n",
    "    print(f\"MRR:  {t2i_mrr/N:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Image to Text (I2T) ---\")\n",
    "    print(f\"R@1:  {i2t_r1/N*100:.2f}% | R@5: {i2t_r5/N*100:.2f}% | R@10: {i2t_r10/N*100:.2f}%\")\n",
    "    print(f\"MRR:  {i2t_mrr/N:.4f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "\n",
    "# --- ACTION: Run on Baseline 3 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluate_cross_encoder(model, test_subset, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Baseline 2: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CLIPDualEncoder(nn.Module):\n",
    "    def __init__(self, img_dim=768, hidden_dim=768, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- TOWER 1: TEXT ---\n",
    "        # Frozen BERT\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for p in self.bert.parameters(): \n",
    "            p.requires_grad = False \n",
    "            \n",
    "        # Text Projector: BERT(768) -> Shared Space(256)\n",
    "        self.txt_proj = nn.Sequential(\n",
    "            nn.Linear(768, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # --- TOWER 2: IMAGE ---\n",
    "        # Image Projector: ImgFeat(768) -> Shared Space(256)\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(img_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Learnable Temperature parameter (starts at ln(1/0.07) ~= 2.65)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * 2.65)\n",
    "\n",
    "    def encode_text(self, input_ids, mask):\n",
    "        # 1. BERT\n",
    "        with torch.no_grad():\n",
    "            bert_out = self.bert(input_ids, mask).last_hidden_state\n",
    "        \n",
    "        # 2. Mean Pooling\n",
    "        mask_expanded = mask.unsqueeze(-1)\n",
    "        sum_embeddings = torch.sum(bert_out * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        \n",
    "        # 3. Project & Normalize\n",
    "        return F.normalize(self.txt_proj(pooled), dim=-1)\n",
    "\n",
    "    def encode_image(self, img_feat):\n",
    "        # 1. Project & Normalize\n",
    "        return F.normalize(self.img_proj(img_feat), dim=-1)\n",
    "\n",
    "    def forward(self, img_feat, input_ids, mask):\n",
    "        # Get embeddings\n",
    "        img_emb = self.encode_image(img_feat)\n",
    "        txt_emb = self.encode_text(input_ids, mask)\n",
    "        return img_emb, txt_emb\n",
    "\n",
    "print(\"CLIPDualEncoder Class Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "clip_model = CLIPDualEncoder().to(device)\n",
    "\n",
    "# Optimizer: Only train the new projection layers\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, clip_model.parameters()), \n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Loss Function: Standard Cross Entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "print(\"Starting CLIP Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    clip_model.train()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # --- 1. Extract Data ---\n",
    "        # (Using dictionary extraction like before)\n",
    "        if isinstance(batch, dict):\n",
    "            img_feat = batch['img_feat'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "        else:\n",
    "            img_feat, _, _ = batch # Adjust if tuple structure differs\n",
    "            # Assuming you handle tuple correctly or use dict\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- 2. Forward Pass ---\n",
    "        # Get normalized embeddings\n",
    "        img_emb, txt_emb = clip_model(img_feat, input_ids, mask)\n",
    "        \n",
    "        # --- 3. Contrastive Loss Calculation ---\n",
    "        # Matrix Multiplication: [Batch, Dim] @ [Dim, Batch] -> [Batch, Batch]\n",
    "        # This gives similarity scores for EVERY pair in the batch\n",
    "        logits = torch.matmul(img_emb, txt_emb.t()) * clip_model.logit_scale.exp()\n",
    "        \n",
    "        # The correct match for Image 0 is Text 0, Image 1 is Text 1...\n",
    "        batch_size = img_feat.size(0)\n",
    "        labels = torch.arange(batch_size).to(device)\n",
    "        \n",
    "        # Symmetric Loss: (Image->Text) + (Text->Image)\n",
    "        loss_i2t = criterion(logits, labels)\n",
    "        loss_t2i = criterion(logits.t(), labels)\n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        \n",
    "        # --- 4. Backprop ---\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} Complete. Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_clip_on_fixed_set(model, subset, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Unpack the exact same 200 items used for Cross-Encoder\n",
    "    imgs = subset[\"img\"].to(device)\n",
    "    ids = subset[\"ids\"].to(device)\n",
    "    masks = subset[\"mask\"].to(device)\n",
    "    N = subset[\"N\"]\n",
    "    \n",
    "    print(f\"\\n--- EVALUATING CLIP (Baseline 2) on Fixed Subset (N={N}) ---\")\n",
    "    \n",
    "    # Initialize metrics for both T2I and I2T\n",
    "    t2i_r1, t2i_r5, t2i_r10, t2i_mrr = 0, 0, 0, 0\n",
    "    i2t_r1, i2t_r5, i2t_r10, i2t_mrr = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Encode All Images [N, Dim]\n",
    "        img_embs = model.encode_image(imgs)\n",
    "        \n",
    "        # 2. Encode All Texts [N, Dim]\n",
    "        txt_embs = model.encode_text(ids, masks)\n",
    "\n",
    "        # 3. Compute Similarity Matrix [N, N]\n",
    "        # S[i, j] = Score for Text_i vs Image_j\n",
    "        sim_matrix = torch.matmul(txt_embs, img_embs.T)\n",
    "        \n",
    "    # --- 4. T2I Metrics (Text to Image) ---\n",
    "    # Query: Text (Rows) | Database: Images (Columns)\n",
    "    \n",
    "    # Sort descending (Best matches first). Sorting over dim=1 (the image database)\n",
    "    t2i_sorted_indices = torch.argsort(sim_matrix, dim=1, descending=True)\n",
    "    \n",
    "    for i in range(N):\n",
    "        # The correct image for Text i is Image i\n",
    "        # Find the rank of index 'i' in the sorted list for row i\n",
    "        rank = (t2i_sorted_indices[i] == i).nonzero(as_tuple=True)[0].item()\n",
    "        true_rank = rank + 1\n",
    "        \n",
    "        if true_rank == 1: t2i_r1 += 1\n",
    "        if true_rank <= 5: t2i_r5 += 1\n",
    "        if true_rank <= 10: t2i_r10 += 1\n",
    "        t2i_mrr += 1.0 / true_rank\n",
    "\n",
    "    # --- 5. I2T Metrics (Image to Text) ---\n",
    "    # Query: Image (Columns) | Database: Texts (Rows)\n",
    "    \n",
    "    # We transpose the matrix mentally and sort over dim=0 (the text database)\n",
    "    # Equivalently, we sort the columns of the original matrix.\n",
    "    i2t_sorted_indices = torch.argsort(sim_matrix, dim=0, descending=True)\n",
    "    \n",
    "    for j in range(N):\n",
    "        # The correct text for Image j is Text j\n",
    "        # Find the rank of index 'j' in the sorted list for column j\n",
    "        rank = (i2t_sorted_indices[:, j] == j).nonzero(as_tuple=True)[0].item()\n",
    "        true_rank = rank + 1\n",
    "        \n",
    "        if true_rank == 1: i2t_r1 += 1\n",
    "        if true_rank <= 5: i2t_r5 += 1\n",
    "        if true_rank <= 10: i2t_r10 += 1\n",
    "        i2t_mrr += 1.0 / true_rank\n",
    "    \n",
    "    # --- 6. Final Output ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"     CLIP RETRIEVAL RESULTS (N={N})\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"--- Text to Image (T2I) ---\")\n",
    "    print(f\"R@1:  {t2i_r1/N*100:.2f}% | R@5: {t2i_r5/N*100:.2f}% | R@10: {t2i_r10/N*100:.2f}%\")\n",
    "    print(f\"MRR:  {t2i_mrr/N:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Image to Text (I2T) ---\")\n",
    "    print(f\"R@1:  {i2t_r1/N*100:.2f}% | R@5: {i2t_r5/N*100:.2f}% | R@10: {i2t_r10/N*100:.2f}%\")\n",
    "    print(f\"MRR:  {i2t_mrr/N:.4f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# --- RUN EVALUATION ---\n",
    "# Assuming 'clip_model' is your CLIP model instance\n",
    "# Assuming 'test_subset' still exists and DEVICE is defined\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluate_clip_on_fixed_set(clip_model, test_subset, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "###testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INVESTIGATION CELL ---\n",
    "batch = next(iter(train_loader))\n",
    "img_feat = batch['img_feat']\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "print(\"--- DATA INSPECTION ---\")\n",
    "print(f\"Image Feature Stats: Min={img_feat.min().item():.4f}, Max={img_feat.max().item():.4f}, Mean={img_feat.mean().item():.4f}\")\n",
    "print(f\"Are Image Features all Zero? {torch.all(img_feat == 0).item()}\")\n",
    "\n",
    "print(f\"Input IDs Stats: Min={input_ids.min().item()}, Max={input_ids.max().item()}\")\n",
    "print(f\"Are Input IDs all Zero? {torch.all(input_ids == 0).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
