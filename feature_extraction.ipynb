{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRRP80FQD5qX",
    "outputId": "730330b3-38b3-4f5a-e54b-19f279471083"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Choose a folder in your Drive where everything for this project will live\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/co_attention_flickr30k_new\"\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "\n",
    "HF_CACHE_DIR = os.path.join(PROJECT_ROOT, \"hf_cache\")\n",
    "os.makedirs(HF_CACHE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swKluLB6D6Dz"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers torchvision tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YG30WsoD6GG",
    "outputId": "024368ce-5e45-4e83-e4e5-8695273b3296"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# List of all Parquet shards for the TEST subset\n",
    "DATA_FILES = [\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0000.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0001.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0002.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0003.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0004.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0005.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0006.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0007.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0008.parquet\",\n",
    "]\n",
    "\n",
    "# When you pass a *list* of files to the \"parquet\" builder, it makes a single split called \"train\"\n",
    "flickr_all = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=DATA_FILES,\n",
    "    cache_dir=HF_CACHE_DIR,\n",
    ")[\"train\"]  # this is the only split name the parquet builder uses in this case\n",
    "\n",
    "print(flickr_all)\n",
    "print(flickr_all[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RA3ipqVD6Iq",
    "outputId": "03a560b8-2bf5-40b3-b69e-3dbb3b51c420"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def is_split(example, name):\n",
    "    return example[\"split\"] == name\n",
    "\n",
    "flickr_train = flickr_all.filter(lambda ex: is_split(ex, \"train\"))\n",
    "flickr_val   = flickr_all.filter(lambda ex: is_split(ex, \"val\"))\n",
    "flickr_test  = flickr_all.filter(lambda ex: is_split(ex, \"test\"))\n",
    "\n",
    "flickr = DatasetDict({\n",
    "    \"train\": flickr_train,\n",
    "    \"validation\": flickr_val,\n",
    "    \"test\": flickr_test,\n",
    "})\n",
    "\n",
    "print(flickr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SG06YdGD6LD",
    "outputId": "60e25aca-cbbb-4e3c-d549-2c70780161b4"
   },
   "outputs": [],
   "source": [
    "print(len(flickr[\"train\"]), len(flickr[\"validation\"]), len(flickr[\"test\"]))\n",
    "print(flickr[\"train\"][0].keys())\n",
    "print(flickr[\"train\"][0][\"caption\"])  # list of 5 captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "AiQnQwm1D6Nv",
    "outputId": "12c97c24-6ec8-426e-f111-00c0658c24d1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example = flickr[\"train\"][0]\n",
    "img = example[\"image\"]          # PIL Image\n",
    "captions = example[\"caption\"]   # list of 5 strings\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(captions[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "34ccb17324434958a26a16c927055516",
      "10b5bbfb82374ccc97aec400bf733edc",
      "6c21bc20d12749b8adf09a0facff5bed",
      "0896a5f372324127a10a235fa747f557",
      "1d6c4285196242ebb5ef09b13a4fb322",
      "df2714ddd5d1441dbab8646442de0ac0",
      "8c2de5a8ee5a40dcb89cc5ebd3fe407d",
      "18dc576226fb463492f8118feebe5210",
      "98dcdabae5434f668e3af5283dc1f3f7",
      "290c1c4268be4ae4b61ce39dcc678c02",
      "adf0e098b0a8490695e2b9f2e766d12f",
      "6fad035c285f46899dd84a6cacd511a6",
      "bd414ade89214072b04fa18fb4c8dac6",
      "a13e633a24634758a88965d77085bf1d",
      "b953d9e800744c1196fd7ad47b22a967",
      "b6c6c7072bde43bd9b659252510ace67",
      "c1c51bc44a9346c8a15d781a2780983e",
      "63c2b8519545405aac5845791d95d318",
      "6d83f5395e4e47b3a40dc8466e1a3776",
      "bc20d8d11ed9485493be672018bed77d",
      "1aa7e7b2bd534a2a86a5a6e2b4c20ff7",
      "7f1bad3284e64993904e0eb28d65417b",
      "896c63858b5349e691e14c5acedf0fc7",
      "a0168f0e6aa240518291ec4d96061320",
      "8cd5d10731fc4b679bd335b4d6d963f0",
      "efbd937fd62443eb9c5d59114d4e4955",
      "2132e926de3e41f49c723a114495c548",
      "27f56d2099dc43979b5d7b0af6ac97ec",
      "7a32f9d6f98f4feb8148b79c236f4a35",
      "6b520aefca4746b99d658ef75ef6e738",
      "cf47a3229a904d54a32dfec1df6f26fd",
      "c2551f6414154e569bf2582d8f6b519b",
      "af1e8086ee9d4e3ea41e7c59c10d054f",
      "4228b843ee994d06b739add71ae0e563",
      "d3d0598ba7f34f44bffbd11fae4c8c4d",
      "37a7160672da4216a474015d927910b5",
      "bda70ae489fe403b8d6534c30d9d3f77",
      "3c221f4983d64f1ba7275882e74c16e0",
      "8bea1757ad3540a3b58d9ff13ae0fdfe",
      "82781c9f767148c69c1615b7e27afbdd",
      "a00075120ecf4507a4c528d3235548aa",
      "854d570566a4456b8a4b2b8fd63e7a8e",
      "00d40fd757af4c1b82a9375e6dec7dc7",
      "1a71ac5764d445dc853af4da93d3b2af"
     ]
    },
    "id": "KhatCwaBD6QS",
    "outputId": "5e0c5854-a980-4b34-b32a-cf82d1dbcf76"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UcGIR2hGktl",
    "outputId": "199bc52d-6e4f-4f7c-dd4a-c4655df32e2b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "vit_weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit = models.vit_b_16(weights=vit_weights)\n",
    "\n",
    "# Replace classification head with identity so vit(x) returns features\n",
    "vit.heads = nn.Identity()\n",
    "for p in vit.parameters():\n",
    "    p.requires_grad = False\n",
    "vit.to(device)\n",
    "vit.eval()\n",
    "\n",
    "# Preprocessing pipeline that matches the ViT weights\n",
    "vit_preprocess = vit_weights.transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOYZEfG8GkwF"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, image_transform, tokenizer, max_length=32,\n",
    "                 random_caption=False):\n",
    "        self.ds = hf_dataset\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.random_caption = random_caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[idx]\n",
    "\n",
    "        # ---- image ----\n",
    "        img = ex[\"image\"].convert(\"RGB\")  # HF Image -> PIL\n",
    "        pixel_values = self.image_transform(img)  # tensor [3, H, W]\n",
    "\n",
    "        # ---- caption ----\n",
    "        captions = ex[\"caption\"]  # list of 5 strings\n",
    "        if self.random_caption:\n",
    "            caption = random.choice(captions)\n",
    "        else:\n",
    "            caption = captions[0]  # first caption only\n",
    "\n",
    "        tok = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,                       # [3, H, W]\n",
    "            \"input_ids\": tok[\"input_ids\"].squeeze(0),           # [max_len]\n",
    "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0), # [max_len]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2v2Nt2s4Gkyj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_pt = Flickr30kDataset(\n",
    "    flickr[\"train\"], image_transform=vit_preprocess,\n",
    "    tokenizer=tokenizer, max_length=MAX_LEN, random_caption=True\n",
    ")\n",
    "val_pt = Flickr30kDataset(\n",
    "    flickr[\"validation\"], image_transform=vit_preprocess,\n",
    "    tokenizer=tokenizer, max_length=MAX_LEN, random_caption=False\n",
    ")\n",
    "test_pt = Flickr30kDataset(\n",
    "    flickr[\"test\"], image_transform=vit_preprocess,\n",
    "    tokenizer=tokenizer, max_length=MAX_LEN, random_caption=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_pt, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_pt, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vf2tobZnGk1G",
    "outputId": "41568ef3-8ef7-438e-bf57-2d41c13d74e4"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"pixel_values\"].shape)   # [B, 3, 224, 224] (or similar)\n",
    "print(batch[\"input_ids\"].shape)      # [B, MAX_LEN]\n",
    "print(batch[\"attention_mask\"].shape) # [B, MAX_LEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9ndaqEvGk3H"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader_feats = DataLoader(\n",
    "    train_pt, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_loader_feats = DataLoader(\n",
    "    val_pt, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader_feats = DataLoader(\n",
    "    test_pt, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtalaOnpHaBE",
    "outputId": "b9289e36-e640-4f7e-fb4f-ed1a0e9018d0"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vit_weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit = models.vit_b_16(weights=vit_weights)\n",
    "\n",
    "vit.heads = nn.Identity()        # removing classification head\n",
    "for p in vit.parameters():\n",
    "    p.requires_grad = False\n",
    "vit.to(device)\n",
    "vit.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArKAGyF30Kxu"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "542f18b001a14e0e8efd4b8c685a67c1",
      "ae6393b497024263bc167c5aec89e85a",
      "e3aa09caa8af4788a5712bd95f0d9ad6",
      "d3ac43a4fa4647c9bc01142fcdeec05f",
      "3ae6b17c8d2e4e6ea2180c5612f714be",
      "71deb97020164d9e92339deb869191e4",
      "24c917256e2e4f0c8e93b11712fd060b",
      "7950b4c9ecd9477684efdfd0632587e7",
      "a8cab5aa581d4f6cab09c00b26f5b77b",
      "6d4878996c034d0db5a44180f2d74b51",
      "4ab1f636697b4e3d8207a9e5f60076ce",
      "f998d543a4564815a777fc6a24ac21f4",
      "3539bde2ed0146478f771c0c42719093",
      "780f170e40c547ac899e1532deb883a2",
      "019b4dd0a7864b9eab79c1a9ff974f8b",
      "311cb4b3e6084d6481ad9dfdd07a1ed3",
      "06a7d3123d974cc3bb240745df359139",
      "37143bf6f33740a8b1d998e31688946e",
      "4cd25faaaa2a4ffb9e424dadbf801156",
      "6eea05fdd5b143ed8a915e960834d763",
      "4ce8b3e35d624feb866a3632c3be9670",
      "35d7773758e346e69fdb605a1814cf3b",
      "5e321d47277b41e685e45676aab263c5",
      "d19a2ef454894720a057b57504370266",
      "6ec5a2e534bb41faa6e8b751e9eb5212",
      "1e28654837c0488284f3f9d83a148fe5",
      "cf838efc6a6d45a8bd35652229a59c05",
      "7a3e21eb6e80493c9b2857914a7804c3",
      "e61412b545464dfc80b2dd3f8cd735c6",
      "ab0094c355874052b1734043e6c140a3",
      "4175a83fc99a49a986e340b9a3878ef3",
      "2840d7b76cac45908242c0f216821389",
      "8b2efa1b990847b9b81233a0f3e15463"
     ]
    },
    "id": "qYpZNEkZ-FmD",
    "outputId": "639fb421-a4a6-44e8-a6f3-af25d47572d8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure the output directory exists\n",
    "feat_dir = os.path.join(PROJECT_ROOT, \"features_vit_b16\")\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "\n",
    "def extract_and_save_features(dataloader, split_name):\n",
    "    print(f\"Starting extraction for: {split_name}\")\n",
    "\n",
    "    global_feats_list = []\n",
    "    patch_feats_list = []\n",
    "\n",
    "    vit.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Extracting {split_name}\"):\n",
    "            imgs = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "            # Input: [B, 3, 224, 224] -> Output: [B, 768, 14, 14]\n",
    "            x = vit.conv_proj(imgs)\n",
    "\n",
    "            # [B, 768, 196] -> [B, 196, 768]\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "            # 2. Append CLS Token\n",
    "            batch_size = x.shape[0]\n",
    "            # Create a [B, 1, 768] CLS token\n",
    "            batch_class_token = vit.class_token.expand(batch_size, -1, -1)\n",
    "            # Concatenate: [B, 197, 768]\n",
    "            x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "            # 3. Encoder Pass (Transformer Layers)\n",
    "            # This applies self-attention across patches\n",
    "            x = vit.encoder(x)\n",
    "\n",
    "            # --- SEPARATE OUTPUTS ---\n",
    "\n",
    "            # Global Feature: Just the 1st token [CLS]\n",
    "            # Shape: [B, 768]\n",
    "            cls_token = x[:, 0]\n",
    "\n",
    "            # Patch Features: All 197 tokens (including CLS)\n",
    "            # Shape: [B, 197, 768]\n",
    "            patch_tokens = x\n",
    "\n",
    "            # --- STORE RESULTS ---\n",
    "            global_feats_list.append(cls_token.cpu())\n",
    "\n",
    "            patch_feats_list.append(patch_tokens.cpu().half())\n",
    "\n",
    "    # Concatenate all batches into one large tensor\n",
    "    all_global = torch.cat(global_feats_list, dim=0)\n",
    "    all_patches = torch.cat(patch_feats_list, dim=0)\n",
    "\n",
    "    # Save filenames\n",
    "    global_path = os.path.join(feat_dir, f\"flickr30k_{split_name}_global.pt\")\n",
    "    patch_path = os.path.join(feat_dir, f\"flickr30k_{split_name}_patch.pt\")\n",
    "\n",
    "    print(f\"Saving Global Features {all_global.shape} to {global_path}\")\n",
    "    torch.save(all_global, global_path)\n",
    "\n",
    "    print(f\"Saving Patch Features {all_patches.shape} to {patch_path}\")\n",
    "    torch.save(all_patches, patch_path)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Run for all 3 splits using the existing dataloaders\n",
    "extract_and_save_features(train_loader_feats, \"train\")\n",
    "extract_and_save_features(val_loader_feats, \"val\")\n",
    "extract_and_save_features(test_loader_feats, \"test\")\n",
    "\n",
    "print(\"All features extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "yaoDhimNHaF5",
    "outputId": "b35e6df0-d65d-462b-975a-425face4e2e5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "three files in Drive:\n",
    "\n",
    "flickr30k_train_vit_b16.pt – shape [29000, D]\n",
    "\n",
    "flickr30k_val_vit_b16.pt – shape [1014, D]\n",
    "\n",
    "flickr30k_test_vit_b16.pt – shape [1000, D]\n",
    "\n",
    "where D is the ViT embedding size (e.g. 768 for vit_b_16)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdK1YOnYfoHt",
    "outputId": "965ffaaa-81d9-447a-c6a1-6799b9d85090"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the path where features were saved\n",
    "feat_dir = os.path.join(PROJECT_ROOT, \"features_vit_b16\")\n",
    "\n",
    "# Load the tensors from disk\n",
    "img_feats_train = torch.load(os.path.join(feat_dir, \"flickr30k_train_global.pt\"))\n",
    "img_feats_val   = torch.load(os.path.join(feat_dir, \"flickr30k_val_global.pt\"))\n",
    "img_feats_test  = torch.load(os.path.join(feat_dir, \"flickr30k_test_global.pt\"))\n",
    "\n",
    "print(f\"Train feats shape: {img_feats_train.shape}\")\n",
    "print(f\"Val feats shape:   {img_feats_val.shape}\")\n",
    "print(f\"Test feats shape:  {img_feats_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlXBSc3FHaKG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class Flickr30kRetrievalDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, img_feats, tokenizer, max_length=32,\n",
    "                 random_caption=True):\n",
    "        assert len(hf_dataset) == img_feats.size(0)\n",
    "        self.ds = hf_dataset\n",
    "        self.img_feats = img_feats\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.random_caption = random_caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[idx]\n",
    "\n",
    "        # precomputed image feature\n",
    "        img_feat = self.img_feats[idx]          # [768]\n",
    "\n",
    "        # pick a caption\n",
    "        captions = ex[\"caption\"]                # list of 5 strings\n",
    "        if self.random_caption:\n",
    "            caption = random.choice(captions)\n",
    "        else:\n",
    "            caption = captions[0]\n",
    "\n",
    "        tok = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"img_feat\": img_feat,                              # [768]\n",
    "            \"input_ids\": tok[\"input_ids\"].squeeze(0),          # [max_len]\n",
    "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0),# [max_len]\n",
    "            \"caption\": caption,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_0ZgqetHaNw",
    "outputId": "212cbf8a-b883-4d04-dc4f-bd9420211a84"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  # can be larger now because we use features, not images\n",
    "\n",
    "train_ret = Flickr30kRetrievalDataset(\n",
    "    flickr[\"train\"], img_feats_train, tokenizer,\n",
    "    max_length=MAX_LEN, random_caption=True\n",
    ")\n",
    "val_ret = Flickr30kRetrievalDataset(\n",
    "    flickr[\"validation\"], img_feats_val, tokenizer,\n",
    "    max_length=MAX_LEN, random_caption=False\n",
    ")\n",
    "\n",
    "train_loader_ret = DataLoader(train_ret, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader_ret   = DataLoader(val_ret, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "batch = next(iter(train_loader_ret))\n",
    "print(batch[\"img_feat\"].shape)        # [B, 768]\n",
    "print(batch[\"input_ids\"].shape)       # [B, MAX_LEN]\n",
    "print(batch[\"attention_mask\"].shape)  # [B, MAX_LEN]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
