{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b61iMrlQzKya",
    "outputId": "7bbc83e9-e2dd-429a-e5ed-8fb1b53fdf92"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/co_attention_flickr30k_new/features_vit_b16\"\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "\n",
    "HF_CACHE_DIR = os.path.join(PROJECT_ROOT, \"hf_cache\")\n",
    "os.makedirs(HF_CACHE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnsOk6bW0-_K"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers torchvision tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548,
     "referenced_widgets": [
      "eb0a2f7c335c46ef9b0837f96cc26df3",
      "99974bc6d09344dab60efed989222ab2",
      "707085956b6c4989b0da61e032060d62",
      "abc276abbfbb41c0b155a0efba753429",
      "8fb07be2cfdc4ca0b8ba2b8a2dbaf3de",
      "e907a8a513e3416899c3284fbe032811",
      "2a5b97730c3f46d79986c1a04b2c99d3",
      "44972a2bd467488bb375b2fe7ae0a089",
      "54cd81bec10748c1a2bf34fdf3d08967",
      "f5d4e0683c924bce9002c6712117d76c",
      "ae0d524747524c5abf2406c3e3bb1896",
      "b5879c59b645469082efcf0fabdc0bf9",
      "82e453f6391e413690e511383eb74372",
      "11cbf3cf4b3f453eac0abdddd8edb654",
      "44b6f43e65d44e6cb4cfa0bae1ab0637",
      "a8ffb5e5575f494c95add2bb2d1909ac",
      "09857efeb3564d70ab49814ddebca66b",
      "c53ae0d911cd4bdbad3ad18de78acc1a",
      "85ccaa7dd7f34260a1ab814a0c0cc9e8",
      "984e1399ad4d45dcb06154588e562132",
      "615c145dce13471ebd359f7128dfe236",
      "f90eead6fe434b78b15ccdcccba04a2c",
      "3a261b2aa6d24de190f69525edd3457c",
      "9d34225dbc2e43e193a967ddd9f1fe50",
      "8614fd57443b4edabb43e0f93bcd034a",
      "ca87dd76c69b48908fb69141dfcc219e",
      "d307046713a54f7682827932f8d529f4",
      "f2c943c34ffb4517970164c0ba24c37e",
      "da222cdab57d442ab9c69d148c6f34bc",
      "5886410f10ce4866ab27e73524227193",
      "386b148797fa4e24a58de9eef423838f",
      "44be1ab53c2844a297c64b423bc67d12",
      "e53fbd15d2734ca5be177ce2d1d556cb",
      "b26dcc00ef194319a13658ab27bc000b",
      "da8a8eff53a749cea3400e99b6533b04",
      "e19cddad4bac46aaa31e62f9cf1a67f3",
      "229384d707de485e982768349bea13b9",
      "c050ba256320411a8b30579074796e1b",
      "491d86a916f34869ada3c0a2a797c204",
      "80b47bd1b6994ee7a00a093549d864ec",
      "4157003abc7040aab7bde457bc68303a",
      "9b0d3903da5d407091f94dfdfb286688",
      "b2e95c04647c4c7d8930a81565f96464",
      "4fdfead82a1846d59bff8c49e531287c",
      "348331e8557749209d1e48e1d536888f",
      "ea883bda2883416eb1d18e26e0727c6b",
      "0995fd66f86a41cf8264ac4a19bfc7c2",
      "dba6daeaa48c4ad98ba266100aa86dbf",
      "d6481c75b53b4cb2afb8f65147165f84",
      "84de7ddcee24459a8194d952f0a899ca",
      "a1a2e3df27b34e0794db44ec70964e5f",
      "837e188518384cb7a322de9a4cac9873",
      "9490a0f3e4e948bcb529c5d996daea22",
      "0edaf01e2ca646a1be12e1c29450b793",
      "37c81dec0bdf45519d3fcf670c5ea504",
      "3dc8ed64ec7c4379acda505f60b62f46",
      "aed4929d727e43168c0f76513a1a8e16",
      "4e52addf5a4a4eb38daa66192eb0f926",
      "8511f1b6cb1c4742933931281f780f17",
      "a44612b46f704fae940d8ce233a5db4b",
      "62daec44093b446e8d3ebab4fba06a08",
      "cb07f59ea6f547ef82f5a3305aa4a5d7",
      "98abbaaa785b4fd298436e6738249ea8",
      "54c78570c4d0484eb0caf88b3d0beeab",
      "79ab8926afc04b43a947d7024230dec8",
      "1469a970b44b4cadbedf51eda142b228",
      "3fd2f0eeb00c4f199c7989cd665a83b6",
      "5e069c526d1746efac2e9ce9be73aa67",
      "c325d0823eb74f6fa63691734c24eeeb",
      "06afd6e140fd4f3fb7f54e9cb307fd6b",
      "73f33598823e4d178980e374db7b8946",
      "1c7ab594a69b41dbaceecca19454ae53",
      "5e31632312fa4401857894d54b3dcc10",
      "6d40484fd3f24a38a2e295a98adba4fc",
      "238e8f440d65466ea0b3b4e8e4bddb80",
      "a609997c8d3d4fbcb07f29750eecc631",
      "bbea49f4d3f348d0b1c178052c74d84a",
      "22fde716cf7a45338eeee065ad59f456",
      "29d39e1ff1374e138f2529ac55960000",
      "961edcd5ccfe4c0491f7630900ddbb14",
      "73f4e7f7db2a4904b643a3300a49dac6",
      "458e7175093a45069a4d0c9dfda154be",
      "aaa574664af44f6d94315f85af390ff4",
      "bce3c31228ad44f7971d81917af33367",
      "588b7eb94b154ff5bffcf1a66405bfa0",
      "7781591e6ddc48a3ae9190ebbcdf27b4",
      "3e4ad34b397f4fb1b280951318f1e488",
      "92784850fd6a473897854343078db592",
      "6b713a98d63447e0b735fa94d4fadea1",
      "72f019845c59495cae60145e975a3937",
      "7084e3a24c6e4701bc2e5ee3758150a3",
      "97e2b30373384113add9b6700517732d",
      "a9a92d7f7a6d412cb1bb1a38ad07c5b8",
      "da63b5895b6d4130916b88cb32593899",
      "bc880f166f71497b8fd3d84478890183",
      "6663cbb76daf4d47b4802edbdaac4942",
      "4dd97188382e49c684b92cd9c82849fd",
      "36c38ffb1b704fc2895510e99d015331",
      "ff54125d518a4be8a427c383fab806b2",
      "9d05f10c02284ab9b9ef148095c03edd",
      "f11ae7671e46467fb185e6c46e69bd1b",
      "000ccf292d074bedb5e260cd7db27796",
      "0aef6b64654b4a4ab6210e6d3f5ac25c",
      "12683bf8fa36440582c9a8d07ffd53c8",
      "9288c4b16c7e46deb572aa7d1779f1a4",
      "7d7c838d6d8e422393bcf0a1e426bd36",
      "c4d11a251e4640b6a3c717e80fc10d00",
      "7d8c6658fcf24a08905117bf2f337a19",
      "6acd595844054a869276148beb228b40",
      "9cfc09e12ee742febdc0fae035c7b777"
     ]
    },
    "id": "u8Cf-EWI1ClG",
    "outputId": "5285a680-5d57-4ab5-9eb6-ef9e23e8dadf"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# List of all Parquet shards for the TEST subset\n",
    "DATA_FILES = [\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0000.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0001.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0002.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0003.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0004.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0005.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0006.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0007.parquet\",\n",
    "    \"https://huggingface.co/datasets/nlphuji/flickr30k/resolve/refs%2Fconvert%2Fparquet/TEST/test/0008.parquet\",\n",
    "]\n",
    "\n",
    "\n",
    "flickr_all = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=DATA_FILES,\n",
    "    cache_dir=HF_CACHE_DIR,\n",
    ")[\"train\"]  \n",
    "\n",
    "print(flickr_all)\n",
    "print(flickr_all[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356,
     "referenced_widgets": [
      "660ed4addc8e4aacb874f8464cb3eace",
      "6b80b61df54a4fa38d20b1e1369afb1f",
      "a9c3fe905364481e8c79abcc8e0a0d9e",
      "7cd86702da47413985b96aec5ff10948",
      "a72c9706a8ae4bbcbc7ff88c71f4045b",
      "cdfc01e0f6284af1b1d3042208c99b1a",
      "399998078fc342a39a982ad2a48339f4",
      "d595ce611e2e4e7db769fe11006d6a2e",
      "2c72dec622c8453396f77763c6843b87",
      "0a7ddb15facf45e182364ce493145354",
      "826dfdb01aaa46ef85a6e805d8f377be",
      "5531b500a1e74f418e64a24b061682b8",
      "c835fa9378b748c89116368c1e1c4c7d",
      "9b28222cda374b8ab0a60c57733fefb3",
      "7015a460f19f416989d68929cf32d110",
      "b8740782a1d14db68b6842bad7d9601b",
      "5ac9e4c33e6f4b28a7eb6d963d32bc3a",
      "48ce4681ac4c41f0898ba37084323073",
      "cccd8e26811b49ba830ba03422d1b857",
      "545f797481b849fe843cfeca2770c6a6",
      "50632325914c4be6b3b93c07a6917dc8",
      "edc504fbefeb4b39ae5c1d217eacbdca",
      "7b6940762f07494faf780999a8009ed8",
      "0650545e11334a6abeaf628270e6ea51",
      "c507b7be6a104f41ab2c3f6211cc0ef3",
      "b15f8f0e9b104ff79d7a0b03da46db41",
      "abefbb418e724fccb26a7f4ea9a56057",
      "1784c404508048fab865d28918ba9ff3",
      "97f46a9fe2464e9fa4643dce525fd3c9",
      "2b73098d9b0648aeb2aaad6f2fbb7a2b",
      "9f62326e816f49e3b4312e11add8eb25",
      "6b03ac7291ef4e1b921cab9319928e2f",
      "5f4ff16100cc463bbd9269df27c39db4"
     ]
    },
    "id": "n0o66_381InB",
    "outputId": "b9e925d8-fc24-4f65-dcec-6014ac19390f"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def is_split(example, name):\n",
    "    return example[\"split\"] == name\n",
    "\n",
    "flickr_train = flickr_all.filter(lambda ex: is_split(ex, \"train\"))\n",
    "flickr_val   = flickr_all.filter(lambda ex: is_split(ex, \"val\"))\n",
    "flickr_test  = flickr_all.filter(lambda ex: is_split(ex, \"test\"))\n",
    "\n",
    "flickr = DatasetDict({\n",
    "    \"train\": flickr_train,\n",
    "    \"validation\": flickr_val,\n",
    "    \"test\": flickr_test,\n",
    "})\n",
    "\n",
    "print(flickr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQXjQbby1K58",
    "outputId": "eea94142-a356-44dc-c64d-2d2e3c0f7fff"
   },
   "outputs": [],
   "source": [
    "print(len(flickr[\"train\"]), len(flickr[\"validation\"]), len(flickr[\"test\"]))\n",
    "print(flickr[\"train\"][0].keys())\n",
    "print(flickr[\"train\"][0][\"caption\"])  # list of 5 captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "QJ_xPNZK1Nv8",
    "outputId": "2fb30de7-d5d7-46f8-f190-c0ba0f6aca1f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example = flickr[\"train\"][0]\n",
    "img = example[\"image\"]          # PIL Image\n",
    "captions = example[\"caption\"]   # list of 5 strings\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(captions[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1f1808b39dc34e168314403acdc77e52",
      "6d6e2fd2653345beb3bcd863215bfc62",
      "04533981759f402ca087282afbad36f0",
      "21cc4b5dcc6c4bd9bca09bccfb8607d2",
      "ff7a28d9074b4b2baf4fa64f4c315de4",
      "6af39ddd045c430ea721a489cdc9d3bb",
      "c584a10aae844a3a9bdbd9f4c51fdc44",
      "97c7fcf5b5514d1fb7990479c7b47414",
      "5b166073cd4044aa892994858f6cda9a",
      "67d8805d665d4644acbcd98789ec1ba0",
      "710ccb787eeb49b4bfa0c22868e48a87",
      "98470c0b904b4506afef8450015f3496",
      "69e4db610ce34d08848ddd26f6e67be3",
      "f048e792d3a24c87b8a12f256e56e7d5",
      "5db152da84474fa5bf446cdf06d933ab",
      "98f974ee2c8344b5909420640780b000",
      "86072732b126490ea7af986e1aa0188b",
      "f2de659a990843b1b858e626f914f633",
      "749b16802672413e861a2b6e3a5aae7e",
      "556bb0ca99e84513b37a9709d96cf707",
      "f4302a2ff10243419707ce41dd7aca68",
      "2a52c6c017634dc8bab396a75d17f114",
      "79120a68ce4a484cbf40ad87a70de8e2",
      "029d68abb2ed4a859bb35924246f4f6a",
      "c956f491aa9442249817eab52db18023",
      "0693fec207524b388f5c0f77620927d2",
      "3d8244467971403cb824faf967fbd5c6",
      "2dce9928336b401d9963106136779121",
      "d41ca4574a36462b9a026486b7626d55",
      "3ee213b77d8b4edf8f6860396e1f4987",
      "834aaf16d3ae4c18bec401c866ae3ba7",
      "72cbefba43ae45cf8012ff12b51c876e",
      "98274a21bd044adfa93848111b965e17",
      "4c721708f45c4559b10d2b6f99f386a0",
      "fc41597cb2bb45be995dedb11219ac9e",
      "e5f60d8cec4a4383b9a0c8b805c0ee82",
      "f7308bcbd5ec43da84a3977318b952b9",
      "900db0777da4419da8138843ce7ad99d",
      "77219ec94c6e40f0b0aa57951afe2711",
      "7c0c37517de44da5b0a04b20afd787df",
      "6de8a5b2a4e14eefa47beda2a58d46d4",
      "ef4286a918fc46b5baf4b8f95702e646",
      "ac2824ecf16e4a4bae31ca26711949a5",
      "76dc412754dc449289b79b911a00fbd2"
     ]
    },
    "id": "SBn5Y1Ks1Ocd",
    "outputId": "735ac959-d370-4a4e-b195-2347210802f0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LEN = 32  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsoEEs9W1Qnz",
    "outputId": "e6ce32dd-a0fa-4f80-a584-1ccf9ffa2e76"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "vit_weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit = models.vit_b_16(weights=vit_weights)\n",
    "\n",
    "# Replacing classification head with identity so vit(x) returns features\n",
    "vit.heads = nn.Identity()\n",
    "for p in vit.parameters():\n",
    "    p.requires_grad = False\n",
    "vit.to(device)\n",
    "vit.eval()\n",
    "\n",
    "# Preprocessing pipeline that matches the ViT weights\n",
    "vit_preprocess = vit_weights.transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI0n5EIq1Siu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, image_transform, tokenizer, max_length=32,\n",
    "                 random_caption=False):\n",
    "        self.ds = hf_dataset\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.random_caption = random_caption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[idx]\n",
    "\n",
    "        # image \n",
    "        img = ex[\"image\"].convert(\"RGB\")  # HF Image -> PIL\n",
    "        pixel_values = self.image_transform(img)  # tensor [3, H, W]\n",
    "\n",
    "        # caption \n",
    "        captions = ex[\"caption\"]  # list of 5 strings\n",
    "        if self.random_caption:\n",
    "            caption = random.choice(captions)\n",
    "        else:\n",
    "            caption = captions[0]  # first caption only\n",
    "\n",
    "        tok = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,                       # [3, H, W]\n",
    "            \"input_ids\": tok[\"input_ids\"].squeeze(0),           # [max_len]\n",
    "            \"attention_mask\": tok[\"attention_mask\"].squeeze(0), # [max_len]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GP4Kx-Wa309h"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_pt = Flickr30kDataset(\n",
    "    flickr[\"train\"], image_transform=vit_preprocess,\n",
    "    tokenizer=tokenizer, max_length=MAX_LEN, random_caption=True\n",
    ")\n",
    "val_pt = Flickr30kDataset(\n",
    "    flickr[\"validation\"], image_transform=vit_preprocess,\n",
    "    tokenizer=tokenizer, max_length=MAX_LEN, random_caption=False\n",
    ")\n",
    "test_pt = Flickr30kDataset(\n",
    "    flickr[\"test\"], image_transform=vit_preprocess,\n",
    "    tokenizer=tokenizer, max_length=MAX_LEN, random_caption=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_pt, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_pt, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAiSlK4l1Uqq",
    "outputId": "f1b26845-0689-4a7a-ccd1-7af9ec7a7e4e"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"pixel_values\"].shape)   # [B, 3, 224, 224] (or similar)\n",
    "print(batch[\"input_ids\"].shape)      # [B, MAX_LEN]\n",
    "print(batch[\"attention_mask\"].shape) # [B, MAX_LEN]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4Wc-7_z1XNu"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64  \n",
    "\n",
    "train_loader_feats = DataLoader(\n",
    "    train_pt, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_loader_feats = DataLoader(\n",
    "    val_pt, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader_feats = DataLoader(\n",
    "    test_pt, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbB77a2y1a5o",
    "outputId": "f7b3c72a-3cc5-40b5-daf6-e35b32cb640a"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vit_weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit = models.vit_b_16(weights=vit_weights)\n",
    "\n",
    "vit.heads = nn.Identity()        # removing classification head\n",
    "for p in vit.parameters():\n",
    "    p.requires_grad = False\n",
    "vit.to(device)\n",
    "vit.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "03febfbd8d4f4aee9eefdcfcf8ecb3e7",
      "e5f7cdfd411f49d78c722e11d0eca36e",
      "3702533c6bec4d96afff1afb9ea8fdf3",
      "c5a07eb3c59a4a9c904731a397b6229b",
      "1fd06f1f99d445bca818976c6780c68a",
      "0d0dd89aab80450e96efdb83feb5d06a",
      "59bf08ee2e364294a05c4a5cb08ae14b",
      "4fdd5c16f8de45d7ae31e5a6be8b436a",
      "ef1264bd47ef4da1ad045c2f6cb9245b",
      "2c11abfee4be407c826fea63f87a7210",
      "eb7eb33901894ccb935498aa4d7e8892",
      "5930ef6038704217870109ce0be24b6c",
      "986e95e946d34bfda3fe9818daa0c7d5",
      "5e8d4a991f2b43318bb0c7b094b45b03",
      "6f23a13d23c345978f2372f073ba67d8",
      "f939035469fd4909a97b77aa033780f7",
      "cc03d42edc1449118a4ab8494a2c4f4d",
      "2216d6638a99414888a8e2ee35d53de3",
      "2ae3e596e21d49d480f6237ea0190645",
      "3450b7c562d1445a9df2531282ef2d86",
      "46d53bdbf678424bb738082a201a8bf4",
      "4818c14c7df6499dbc5eaf736ef2114c",
      "069f04267c18421d8ea7a6b81ce067d2",
      "1ef32621b0ec4373bc797aebcbe67248",
      "cc5478d6709d436da82c5512accf1caf",
      "e09ed7a822bc4efd98e07f304319c223",
      "e5c744655c924f92b0f29b711b0e915a",
      "64f33eabb4c54517a1625dfed029ff77",
      "894b4dbcdc094cbe9349a2b373b1cb1d",
      "188c82c3063f4fc4b4ae1579124fb364",
      "f937d04f2d854d06a2e3295b4a486d0e",
      "64b276c2010c46cfa8508e8faf219fe1",
      "f374370fa0f247039d4dd9c3db4d0bcb"
     ]
    },
    "id": "pMAchU8n1da0",
    "outputId": "45bb55a2-0468-4067-b454-b4742947a5e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/co_attention_flickr30k_new/features_vit_b16'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def extract_and_save_new_patch_features(dataloader, split_name):\n",
    "    print(f\"\\nStarting extraction for: {split_name} (Patch Features and Text IDs)\")\n",
    "\n",
    "    patch_feats_list = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    global vit, device\n",
    "    vit.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Extracting {split_name}\"):\n",
    "            imgs = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "            # 1. Patchify (Conv Projection)\n",
    "            x = vit.conv_proj(imgs)\n",
    "            x = x.flatten(2).transpose(1, 2) # [B, 196, D]\n",
    "\n",
    "            # 2. Prepend CLS Token\n",
    "            batch_size = x.shape[0]\n",
    "            batch_class_token = vit.class_token.expand(batch_size, -1, -1)\n",
    "            x = torch.cat([batch_class_token, x], dim=1) # [B, 197, D]\n",
    "\n",
    "            # 3. Add Positional Embedding (REQUIRED for 197 size)\n",
    "            if hasattr(vit, 'pos_embedding'):\n",
    "                x = x + vit.pos_embedding\n",
    "            elif hasattr(vit, 'positional_embedding'):\n",
    "                x = x + vit.positional_embedding\n",
    "\n",
    "            # 5. Run through the Encoder (Transformer Layers)\n",
    "            patch_tokens = vit.encoder(x)\n",
    "\n",
    "            patch_feats_list.append(patch_tokens.cpu().half())\n",
    "\n",
    "            # 2. Collect Text Data\n",
    "            input_ids_list.append(batch[\"input_ids\"].cpu())\n",
    "            attention_mask_list.append(batch[\"attention_mask\"].cpu())\n",
    "\n",
    "    # Concatenate Results and Save Dictionary \n",
    "    all_patches = torch.cat(patch_feats_list, dim=0)\n",
    "    all_ids = torch.cat(input_ids_list, dim=0)\n",
    "    all_masks = torch.cat(attention_mask_list, dim=0)\n",
    "    N = all_patches.shape[0]\n",
    "\n",
    "    data_to_save = {\n",
    "        \"img\": all_patches,\n",
    "        \"ids\": all_ids,\n",
    "        \"mask\": all_masks,\n",
    "        \"N\": N\n",
    "    }\n",
    "\n",
    "    if N != all_ids.shape[0] or all_patches.shape[1] != 197:\n",
    "        raise ValueError(f\"CRITICAL ERROR: Data mismatch. N={N}, Patches={all_patches.shape[1]}. Expected 197.\")\n",
    "\n",
    "    patch_path_new = os.path.join(OUTPUT_DIR, f\"flickr30k_{split_name}_FINAL.pt\")\n",
    "\n",
    "    print(f\"Saving FINAL Data Dictionary (N={N}) to {patch_path_new}\")\n",
    "    torch.save(data_to_save, patch_path_new)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    return patch_path_new\n",
    "\n",
    "# Run for all 3 splits \n",
    "new_train_path = extract_and_save_new_patch_features(train_loader_feats, \"train\")\n",
    "new_val_path   = extract_and_save_new_patch_features(val_loader_feats, \"val\")\n",
    "new_test_path  = extract_and_save_new_patch_features(test_loader_feats, \"test\")\n",
    "\n",
    "print(\"\\n✅ All FINAL data dictionaries extracted successfully and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVUSWZyz6RBA"
   },
   "source": [
    "###CO-ATTENTION TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ifPrzhQ6UEm",
    "outputId": "adda6fa3-6928-4686-b6ee-faae88898109"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU instance.')\n",
    "else:\n",
    "  print('GPU is available:\\n' + gpu_info)\n",
    "\n",
    "# Install necessary libraries\n",
    "!pip install -q transformers torch numpy tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTFcCnLQ80hn",
    "outputId": "d4652f90-93ef-4844-fa92-9df3b1067fec"
   },
   "outputs": [],
   "source": [
    "\n",
    "BASE_DATA_PATH = '/content/drive/MyDrive/co_attention_flickr30k_new/features_vit_b16/'\n",
    "\n",
    "TEST_FILE = BASE_DATA_PATH + 'flickr30k_test_FINAL.pt'\n",
    "TRAIN_FILE = BASE_DATA_PATH + 'flickr30k_train_FINAL.pt'\n",
    "VAL_FILE = BASE_DATA_PATH + 'flickr30k_val_FINAL.pt'\n",
    "\n",
    "def load_and_verify_data(file_path, split_name):\n",
    "    try:\n",
    "        data_dict = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "        img_shape = data_dict[\"img\"].shape\n",
    "        if len(img_shape) != 3:\n",
    "            raise ValueError(f\"Image shape for {split_name} is {img_shape}. Expected 3 dimensions [N, Patches, Dim] for Co-Attention.\")\n",
    "\n",
    "        print(f\"{split_name} data loaded. N={data_dict['N']}, Image Patches Shape: {img_shape}\")\n",
    "        return data_dict\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at {file_path}. Please check the path and file names.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading {split_name} data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the 3 Splits\n",
    "try:\n",
    "    train_data = load_and_verify_data(TRAIN_FILE, \"Train\")\n",
    "    val_data = load_and_verify_data(VAL_FILE, \"Validation\")\n",
    "    test_data = load_and_verify_data(TEST_FILE, \"Test\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    class FixedSubsetDataset(Dataset):\n",
    "        def __init__(self, data_dict):\n",
    "            self.imgs = data_dict[\"img\"]\n",
    "            self.ids = data_dict[\"ids\"]\n",
    "            self.masks = data_dict[\"mask\"]\n",
    "            self.N = data_dict[\"N\"]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.N\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                \"img_feat\": self.imgs[idx],\n",
    "                \"input_ids\": self.ids[idx],\n",
    "                \"attention_mask\": self.masks[idx],\n",
    "            }\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = FixedSubsetDataset(train_data)\n",
    "    val_dataset = FixedSubsetDataset(val_data)\n",
    "\n",
    "    # batch size for training\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "    # Creating the 'test_subset' for N x N Evaluation \n",
    "    TEST_SUBSET_SIZE = 200\n",
    "\n",
    "    test_subset = {\n",
    "        \"img\": test_data[\"img\"][:TEST_SUBSET_SIZE].to(device),\n",
    "        \"ids\": test_data[\"ids\"][:TEST_SUBSET_SIZE].to(device),\n",
    "        \"mask\": test_data[\"mask\"][:TEST_SUBSET_SIZE].to(device),\n",
    "        \"N\": TEST_SUBSET_SIZE\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Loader created: {len(train_loader)} batches\")\n",
    "    print(f\"Test Subset for Evaluation: N={test_subset['N']} (first {TEST_SUBSET_SIZE} samples)\")\n",
    "    print(f\"This matches your other baseline models (Cross-Attention, CLIP)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"Please ensure the 'device' variable is defined by running Cell 1 first.\")\n",
    "except Exception:\n",
    "    # Stop execution if data loading fails\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNSWvicH837U",
    "outputId": "d1eebb95-f7f7-46f0-a86a-c4f0942f5e41"
   },
   "outputs": [],
   "source": [
    "class BiDirectionalCoAttentionModel(nn.Module):\n",
    "    def __init__(self, patch_dim=768, hidden_dim=768, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Text Encoder (Frozen BERT)\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Image Projection\n",
    "        self.patch_proj = nn.Linear(patch_dim, hidden_dim)\n",
    "\n",
    "        # Co-Attention Layers \n",
    "        # Image attends to Text (ALL patches attend to ALL text tokens)\n",
    "        self.i2t_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Text attends to Image (ALL text tokens attend to ALL patches)\n",
    "        self.t2i_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Layer Normalization (for residual connections)\n",
    "        self.ln_img = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_txt = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Pooling projections (AFTER co-attention)\n",
    "        self.img_pool = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.txt_pool = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Final similarity head\n",
    "        self.similarity_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, patch_feat, input_ids, attention_mask):\n",
    "        if patch_feat.dtype == torch.float16:\n",
    "            patch_feat = patch_feat.float()\n",
    "\n",
    "        # Get text embeddings: [B, seq_len, hidden_dim]\n",
    "        with torch.no_grad():\n",
    "            txt_seq = self.bert(input_ids, attention_mask).last_hidden_state\n",
    "\n",
    "        # Project image patches: [B, num_patches, hidden_dim]\n",
    "        img_seq = self.patch_proj(patch_feat)\n",
    "\n",
    "        # BIDIRECTIONAL CO-ATTENTION (Full Sequence) \n",
    "\n",
    "        # Image-to-Text Attention\n",
    "        key_padding_mask_txt = (attention_mask == 0)\n",
    "        img_attended, _ = self.i2t_attn(\n",
    "            query=img_seq,           # [B, num_patches, D] ← FULL SEQUENCE\n",
    "            key=txt_seq,             # [B, seq_len, D]\n",
    "            value=txt_seq,           # [B, seq_len, D]\n",
    "            key_padding_mask=key_padding_mask_txt\n",
    "        )\n",
    "        # Residual connection + LayerNorm\n",
    "        img_attended = self.ln_img(img_attended + img_seq)\n",
    "\n",
    "        # Text-to-Image Attention\n",
    "        txt_attended, _ = self.t2i_attn(\n",
    "            query=txt_seq,           # [B, seq_len, D] \n",
    "            key=img_seq,             # [B, num_patches, D]\n",
    "            value=img_seq            # [B, num_patches, D]\n",
    "        )\n",
    "        # Residual connection + LayerNorm\n",
    "        txt_attended = self.ln_txt(txt_attended + txt_seq)\n",
    "\n",
    "        # POOLING (After Co-Attention to preserve information)\n",
    "\n",
    "        # Pool image patches (mean pooling)\n",
    "        img_pooled = img_attended.mean(dim=1)  # [B, D]\n",
    "        img_pooled = self.img_pool(img_pooled)\n",
    "\n",
    "        # Pool text tokens (masked mean pooling)\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).float()  # [B, seq_len, 1]\n",
    "        txt_sum = (txt_attended * mask_expanded).sum(dim=1)   # [B, D]\n",
    "        txt_count = mask_expanded.sum(dim=1).clamp(min=1)     # [B, 1]\n",
    "        txt_pooled = txt_sum / txt_count\n",
    "        txt_pooled = self.txt_pool(txt_pooled)\n",
    "\n",
    "        # FUSION AND SIMILARITY \n",
    "\n",
    "        # Concatenate modalities\n",
    "        fused = torch.cat([img_pooled, txt_pooled], dim=-1)  # [B, 2D]\n",
    "\n",
    "        # Compute similarity score\n",
    "        score = self.similarity_head(fused).squeeze(-1)  # [B]\n",
    "\n",
    "        return score\n",
    "\n",
    "print(\"✓ CORRECTED Co-Attention Model defined (Full Sequence Attention)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WUjQFXo87nh",
    "outputId": "d207aae3-828d-414d-d118-e3c9930e660c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# InfoNCE Loss \n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, scores_matrix):\n",
    "        B = scores_matrix.size(0)\n",
    "        # Scale by temperature\n",
    "        logits = scores_matrix / self.temperature\n",
    "\n",
    "        # Labels: positive pairs are on the diagonal\n",
    "        labels = torch.arange(B, device=scores_matrix.device)\n",
    "\n",
    "        # Symmetric loss (Image-to-Text + Text-to-Image)\n",
    "        loss_i2t = self.cross_entropy(logits, labels)\n",
    "        loss_t2i = self.cross_entropy(logits.t(), labels)\n",
    "\n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "model = BiDirectionalCoAttentionModel(\n",
    "    patch_dim=768,\n",
    "    hidden_dim=768,\n",
    "    num_heads=8,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=5e-5,  # Conservative learning rate\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-6\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "criterion = InfoNCELoss(temperature=0.07)\n",
    "\n",
    "GRAD_CLIP_VALUE = 1.0\n",
    "\n",
    "def train_loop(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        img_feat = batch[\"img_feat\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        B = img_feat.size(0)\n",
    "\n",
    "        # Compute B x B similarity matrix (all pairs in batch) \n",
    "\n",
    "        # Tile image features: [B, P, D] -> [B*B, P, D]\n",
    "        img_tiled = img_feat.unsqueeze(1).repeat(1, B, 1, 1).reshape(\n",
    "            B*B, img_feat.size(1), img_feat.size(2)\n",
    "        )\n",
    "\n",
    "        # Tile text features: [B, L] -> [B*B, L]\n",
    "        ids_tiled = input_ids.unsqueeze(0).repeat(B, 1, 1).reshape(B*B, -1)\n",
    "        mask_tiled = attention_mask.unsqueeze(0).repeat(B, 1, 1).reshape(B*B, -1)\n",
    "\n",
    "        # Compute all B*B similarity scores\n",
    "        scores = model(img_tiled, ids_tiled, mask_tiled)\n",
    "\n",
    "        # Reshape to similarity matrix: [B, B]\n",
    "        scores_matrix = scores.reshape(B, B)\n",
    "\n",
    "        # Compute InfoNCE loss\n",
    "        loss = criterion(scores_matrix)\n",
    "\n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"\\nWARNING: NaN/Inf loss at batch {batch_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_VALUE)\n",
    "\n",
    "        if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
    "            print(f\"\\nWARNING: Invalid gradients at batch {batch_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    print(f\"Epoch Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "print(\"✓ InfoNCE Loss and Training Loop defined\")\n",
    "print(f\"✓ Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8365b8167c1949d9b0a8b5a2b0afc008",
      "dd83fb0602824d3db9c8905d93ea0a8e",
      "521c2b11733743f2b78457286b1ba98c",
      "c1c3011504d442d9b56765c3a7ed2889",
      "84e72bbc20a24328aa9d5b2aa734e493",
      "4399859296a14e43989e722d6a95cd2c",
      "e7d09ccfe4c6411190159e5cc5c5a4ae",
      "1cef963e11c5488c9ed317dc647ce2a1",
      "3e5974203a444f85bf5da25bb08e3811",
      "c57f2c5389e044f1ba2cc03c64d61e24",
      "1916976c27474a0fab4efdb9f0c26d6e",
      "33423b88c1644b6491aaf99ae54aa0af",
      "bed8556dffce46de932df7a21eef1386",
      "245b2e7fe41241ed82861ebede119fc9",
      "a028b301eb2b438994f58c8f03881598",
      "d4626784c3dd40849693bb3344e7d0b0",
      "e8a183bea805470191cd2aee454b3406",
      "9dec89aff4b94af7a4140fb365580078",
      "0739d5bc8c044500946055861fe67acd",
      "250ca2eb2f4442aa8e3d61ead0fe76d3",
      "ca73ad4025934b14b3b6d9851fb91907",
      "fdf75732e16e43debab4a7d5fd4dcf0e",
      "cb30c954dd7c45caaa7c07abf7fb39c0",
      "db8b5e6412e0469294a0b8bef599cf60",
      "e80a41f9bb3642e693f36474f543e602",
      "1c98673aaa2a4a69acb8edf03ea07c7b",
      "2506c7add71149e5b75a3ee04961b930",
      "242944ae49ab438f942fd0519d686a9a",
      "5b7143eda192455eaf37b8024025e9f6",
      "35a3aea2b5134347baa5c48b40f4f578",
      "c321a23c58f84112ad5c8fe89c5c118e",
      "a3ee6ba2012b469fbcc2484d2425ae3c",
      "d2fba59bb68e4b0da73f6dcecb31132c",
      "d8265c0aaeb84c1bb2c620f7e6c128b2",
      "654ccdf7698f4dc1b5f02b6539b595b5",
      "cbdeb7fe197e456dafcf6d81834ceb0f",
      "fb623b3c94884949bf4c76b26343b45d",
      "d7c02653c48f44dd8ce5591a37403208",
      "52790609cf3549bf94fcf64e7359cdcc",
      "3342da7a06ed40e89bffb3f08f1cfb8d",
      "8aa0a0e82cfd4cdaab577e2bd982ca07",
      "2a76d2e54f45404e8969488d9a263f94",
      "0d985abdaa6946a0b51161e30b9ef4dd",
      "6baf5c1ccc3b429ba55a8d0099be0c6e",
      "72b2fbcafa3b4a8dbb9bf68c86a9e972",
      "2cda7806e3cd420d8198fd0d72aeec3d",
      "30fd094ff9044d519745af4c980ff6c7",
      "3eb3cfe50858401db3b53fc560776463",
      "2c4dd1058437433bbacf8f5f5953d253",
      "6eb9941109ff4dc3bfa005bd5e563790",
      "9d2b2f4c11074ddaa00a68d626ce0219",
      "8b2d0f9c62e24f68928691a22d05bc54",
      "350b58d0665c4b0793a7b8d0aad5c4bd",
      "b6f976618bfa4383a9bc5578370c9154",
      "628fa8f671714432a9e4ff88fc649dc9",
      "bcbd7684ebd1499bad4b5b5833669461",
      "72c50e2d96444409b5e1af938b6e81d0",
      "7cceea47b5fc474a81472e41c2e0875e",
      "27101c7724444856bf108a3320501791",
      "cf30354099cc4b46a41f441ed3d18568",
      "a2c26198ef1d4d46b87be3d5ab5bff28",
      "9b2120f60f02400799e4b26f04cd9370",
      "a539da02f3a74cdf824a63a7d7639d04",
      "4ca7c849892c40da86c434157b6636f7",
      "5580f191c8be4009aa2aa73f40b368ef",
      "7422fbc06a0840e58b13c19a71270b6a",
      "81ed58c3a3d743b2bf5ebec06fda6586",
      "5a86e961663343b59b3339ff0533f723",
      "cb5f5604b8984da794ec43f81421cc97",
      "c0c8ef79907143b095014b81096e1d23",
      "91ca7fde638b4a62ac1b6ff18e1c8a78",
      "3f35d904343d442481a2c5217c36013e",
      "e72513791e4140959056e12bb6ced930",
      "a64f82575d6f48a18fc34df091e2efb5",
      "7449573fcf51418d95146d5fb8d4ac17",
      "a270e896aa9b40f3ad328445a08e6f66",
      "0ca7dae71c4740b1863ad8eadd01914a",
      "7569e81aebc944c8812679fd0d95c2b2",
      "9de3e1fda45d40f49d38c536f1357e49",
      "514e7cc2b58b4023aeaa58e3837c96db",
      "aca302d129374de4aa40f970ed1f1f9f",
      "743cd7e060d444bb97b4642bd9f949a4",
      "1680898b3ee44c58b8ba527667f56519",
      "5a00a532341e41c4b4d350872f915766",
      "188ffb2b254d4647ba74bd5eb019be05",
      "3fa8a4f419924b619a73c7465c81cbf2",
      "adb4a793257b4e559d7cc553f84622c7",
      "0d4263bb19074a50a276ed3f8f831483",
      "66dedb64c619471392006a8d24c4e6a9",
      "fb96b39432084d93a4ed508392a5274b",
      "699fe5ce9614435eb09d7f6e232e035d",
      "49a5b6bbf0ed476c8579ca9853e2d9e4",
      "ffc4c45803a64da6a74f561e8d100e41",
      "104538acdddc4637b45894c5e3df375c",
      "a8498113d12841ea8f1dfe73fae80454",
      "436c6003c6ac4e8bbb325ab630613c89",
      "c7d27e4ffc1447b5a4edefec5722e0f5",
      "023ecff90e4548ac9447d1f7612bf588",
      "c3585990ad214f40a89846b1ba850296",
      "f9cd85fabdfc4b959c2f852910306f2b",
      "eab0171353df4918956601be610eb8f5",
      "9719901a94874dfa8a1b77fb91531925",
      "a38c497669784f15a27c73d7e81ec8c1",
      "80b91a7467ef4fab8305b0a7acb83893",
      "d1f9776824b144f4bb3905ee98e39dde",
      "79cd71e97dd24162a45cf257c34de856",
      "1d201eca8e2c4173adbb0879438f2e6c",
      "14daf555cd08428aa11bf26e1ed443a8",
      "b4e2c28e0c4644759efcab6875bee06a",
      "257312768bc54f8c9665ff32443b35c5"
     ]
    },
    "id": "oeJ46HTZ8-Zn",
    "outputId": "a8ec0988-addc-4db3-b03d-f711a52ef094"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "NUM_EPOCHS = 10  # Start with 10 epochs\n",
    "SAVE_DIR = '/content/drive/MyDrive/co_attention_flickr30k_new/'\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, 'co_attention_model_corrected.pth')\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting Training: CORRECTED Co-Attention Model\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Architecture: Full Sequence Bidirectional Co-Attention\")\n",
    "print(f\"Loss: InfoNCE (in-batch negatives, like CLIP)\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_loop(train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"✓ Best model saved (loss: {best_loss:.4f})\")\n",
    "\n",
    "    # Save checkpoint every 3 epochs\n",
    "    if epoch % 3 == 0:\n",
    "        checkpoint_path = SAVE_PATH.replace('.pth', f'_epoch{epoch}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved: epoch {epoch}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Loss: {best_loss:.4f}\")\n",
    "print(f\"Model saved to: {SAVE_PATH}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pddzcVsLQuD8",
    "outputId": "6cda943d-62a7-448f-dc4d-7a0cd16b6ebd"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_fixed_test_subset(loader, device, num_samples=200):\n",
    "    \"\"\"\n",
    "    Extracts the first N samples from the loader to create a fixed evaluation set.\n",
    "    \"\"\"\n",
    "    all_imgs = []\n",
    "    all_ids = []\n",
    "    all_masks = []\n",
    "\n",
    "    collected = 0\n",
    "    print(f\"Extracting fixed subset of {num_samples} samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Handle Dictionary vs Tuple\n",
    "            if isinstance(batch, dict):\n",
    "                img = batch['img_feat']\n",
    "                ids = batch['input_ids']\n",
    "                mask = batch['attention_mask']\n",
    "            else:\n",
    "                img, ids, mask = batch\n",
    "\n",
    "            all_imgs.append(img)\n",
    "            all_ids.append(ids)\n",
    "            all_masks.append(mask)\n",
    "\n",
    "            collected += img.size(0)\n",
    "            if collected >= num_samples:\n",
    "                break\n",
    "\n",
    "    # Concatenate and trim exactly to num_samples\n",
    "    subset = {\n",
    "        \"img\": torch.cat(all_imgs)[:num_samples].to(device),\n",
    "        \"ids\": torch.cat(all_ids)[:num_samples].to(device),\n",
    "        \"mask\": torch.cat(all_masks)[:num_samples].to(device),\n",
    "        \"N\": num_samples\n",
    "    }\n",
    "\n",
    "    print(f\"Fixed Test Subset Ready. (N={num_samples})\")\n",
    "    return subset\n",
    "\n",
    "# Create test_loader if it doesn't exist\n",
    "if 'test_loader' not in locals():\n",
    "    print(\"Creating test_loader...\")\n",
    "    test_dataset = FixedSubsetDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Create the N=200 test subset (same as other baseline models)\n",
    "test_subset = get_fixed_test_subset(test_loader, device, num_samples=200)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test subset created with N={test_subset['N']}\")\n",
    "print(f\"Ready for evaluation - matches your other baseline models!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2I7lOXvqPQx5",
    "outputId": "a2b1a3da-65c7-4d37-8fc0-e333dbc1d8f9"
   },
   "outputs": [],
   "source": [
    "print(f\"Test subset size N = {test_subset['N']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYkObaiMTmYi",
    "outputId": "53de00a0-008b-4d74-9483-9ee5f650d713"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the saved model\n",
    "SAVE_PATH = '/content/drive/MyDrive/co_attention_flickr30k_new/co_attention_model_final.pth'\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = BiDirectionalCrossAttentionModel(patch_dim=768, hidden_dim=768).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
    "\n",
    "# Convert entire model to float32\n",
    "model = model.float()\n",
    "\n",
    "# Verify all parameters are float32\n",
    "print(\"Checking model dtypes...\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.dtype != torch.float32:\n",
    "        print(f\"WARNING: {name} is {param.dtype}\")\n",
    "\n",
    "print(f\"Model loaded and converted to float32\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cet92itzs4z-",
    "outputId": "fcea9e59-3272-4388-a5f8-b979f8c0d8ee"
   },
   "outputs": [],
   "source": [
    "# Clear CUDA memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete any large tensors\n",
    "if 'img_tiled' in locals():\n",
    "    del img_tiled\n",
    "if 'ids_tiled' in locals():\n",
    "    del ids_tiled\n",
    "if 'mask_tiled' in locals():\n",
    "    del mask_tiled\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"✓ Memory cleared\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483,
     "referenced_widgets": [
      "306c314599f143e3b527f3da9baf71ed",
      "7354404ef0e24c9cbbdaa682ce2ad177",
      "46f00603061641cc89555c5db5befe43",
      "8d9ce59cbdbc4eb9a3fa890113a3e7d3",
      "01a49d63147941e5bd2f2a77d0de8c7c",
      "000a1c36ea5b46568d590df35960e50a",
      "9c14159dd0f44c418d07f8390d1fd9ca",
      "8cc936bec9274eff9c9d7606eca49cb3",
      "aeaf8886356e4884a962f3c68ccc51ad",
      "1a3538b45e94403587bb3bc12db2289b",
      "4c661f8ffc4f4a86b1b7969cf95ba2e4"
     ]
    },
    "id": "I7tdKl0g9FJa",
    "outputId": "17ff8297-a1b7-42ec-b8f1-06014bbc6cd8"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Memory-Efficient Evaluation Script\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def calculate_recall_matrix(sim_matrix, direction=\"i2t\"):\n",
    "    if direction == \"t2i\":\n",
    "        sim_matrix = sim_matrix.t()\n",
    "\n",
    "    n = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "    sim_matrix_np = sim_matrix.cpu().numpy()\n",
    "\n",
    "    for i in range(n):\n",
    "        target_score = sim_matrix_np[i, i]\n",
    "        row_scores = sim_matrix_np[i, :]\n",
    "        rank = (row_scores > target_score).sum() + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    r1 = 100.0 * np.sum(ranks <= 1) / n\n",
    "    r5 = 100.0 * np.sum(ranks <= 5) / n\n",
    "    r10 = 100.0 * np.sum(ranks <= 10) / n\n",
    "\n",
    "    return r1, r5, r10\n",
    "\n",
    "\n",
    "def evaluate_co_attention_model(model, subset, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Move data to CPU first to save GPU memory\n",
    "    img_tensor = subset[\"img\"].cpu()\n",
    "    input_ids_tensor = subset[\"ids\"].cpu()\n",
    "    mask_tensor = subset[\"mask\"].cpu()\n",
    "    N = subset[\"N\"]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING Co-Attention Model (N={N})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    SCORING_BATCH_SIZE = 8  \n",
    "    similarity_matrix = torch.zeros((N, N), device='cpu')\n",
    "\n",
    "    print(f\"\\nComputing {N}x{N} Similarity Matrix...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, N, SCORING_BATCH_SIZE), desc=\"Scoring Matrix\"):\n",
    "            # Get batch\n",
    "            img_batch = img_tensor[i:i + SCORING_BATCH_SIZE].to(device)\n",
    "            B = img_batch.size(0)\n",
    "\n",
    "            row_scores = []\n",
    "\n",
    "            TEXT_CHUNK_SIZE = 20  # Process 20 texts at a time\n",
    "\n",
    "            for j in range(0, N, TEXT_CHUNK_SIZE):\n",
    "                text_end = min(j + TEXT_CHUNK_SIZE, N)\n",
    "                num_texts = text_end - j\n",
    "\n",
    "                # Tile Image: [B, P, D] -> [B*num_texts, P, D]\n",
    "                img_tiled = img_batch.unsqueeze(1).repeat(1, num_texts, 1, 1).reshape(\n",
    "                    B * num_texts, img_batch.size(1), img_batch.size(2)\n",
    "                )\n",
    "\n",
    "                # Get text chunk: [num_texts, L] -> [B*num_texts, L]\n",
    "                ids_chunk = input_ids_tensor[j:text_end].to(device)\n",
    "                mask_chunk = mask_tensor[j:text_end].to(device)\n",
    "\n",
    "                ids_tiled = ids_chunk.unsqueeze(0).repeat(B, 1, 1).reshape(B * num_texts, -1)\n",
    "                mask_tiled = mask_chunk.unsqueeze(0).repeat(B, 1, 1).reshape(B * num_texts, -1)\n",
    "\n",
    "                # Compute scores for this chunk\n",
    "                scores = model(img_tiled, ids_tiled, mask_tiled)\n",
    "                scores_reshaped = scores.reshape(B, num_texts).cpu()\n",
    "                row_scores.append(scores_reshaped)\n",
    "\n",
    "                del img_tiled, ids_tiled, mask_tiled, scores, scores_reshaped\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Concatenate all chunks for this batch of images\n",
    "            batch_scores = torch.cat(row_scores, dim=1)  # [B, N]\n",
    "            similarity_matrix[i:i+B, :] = batch_scores\n",
    "\n",
    "            # Clear memory\n",
    "            del img_batch, row_scores, batch_scores\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL RETRIEVAL RESULTS (N={N})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    sim_matrix_gpu = similarity_matrix.to(device)\n",
    "\n",
    "    # Image-to-Text\n",
    "    i2t_r1, i2t_r5, i2t_r10 = calculate_recall_matrix(sim_matrix_gpu, direction=\"i2t\")\n",
    "    print(f\"\\n--- Image to Text (I2T) ---\")\n",
    "    print(f\"R@1:  {i2t_r1:.2f}%\")\n",
    "    print(f\"R@5:  {i2t_r5:.2f}%\")\n",
    "    print(f\"R@10: {i2t_r10:.2f}%\")\n",
    "\n",
    "    # Text-to-Image\n",
    "    t2i_r1, t2i_r5, t2i_r10 = calculate_recall_matrix(sim_matrix_gpu, direction=\"t2i\")\n",
    "    print(f\"\\n--- Text to Image (T2I) ---\")\n",
    "    print(f\"R@1:  {t2i_r1:.2f}%\")\n",
    "    print(f\"R@5:  {t2i_r5:.2f}%\")\n",
    "    print(f\"R@10: {t2i_r10:.2f}%\")\n",
    "\n",
    "    # Average\n",
    "    avg_recall = (i2t_r1 + i2t_r5 + i2t_r10 + t2i_r1 + t2i_r5 + t2i_r10) / 6\n",
    "    print(f\"\\nAverage Recall: {avg_recall:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return {\n",
    "        \"i2t\": (i2t_r1, i2t_r5, i2t_r10),\n",
    "        \"t2i\": (t2i_r1, t2i_r5, t2i_r10),\n",
    "        \"avg_recall\": avg_recall\n",
    "    }\n",
    "\n",
    "\n",
    "# Clear memory first\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load model\n",
    "SAVE_PATH = '/content/drive/MyDrive/co_attention_flickr30k_new/co_attention_model_corrected.pth'\n",
    "\n",
    "model = BiDirectionalCoAttentionModel(\n",
    "    patch_dim=768,\n",
    "    hidden_dim=768,\n",
    "    num_heads=8,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
    "model = model.float()\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded from: {SAVE_PATH}\\n\")\n",
    "\n",
    "results = evaluate_co_attention_model(model, test_subset, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
